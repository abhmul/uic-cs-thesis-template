\section{Martingales}
\label{martingale:martingales}

We start our initial study of martingales by first setting up some underlying concepts. Again, we let \((\Omega, \mathcal{B}, \mathbb{P})\) be our default probability space.

\begin{definition}[\cite{williams_2006}]
    Let \(T\) be a total ordering and let \(S, \Sigma\) be a measure space. A \textbf{stochastic process} is a random element \(X: \Omega, \mathcal{B} \to S^{T}, \Sigma^{T}\), where \(\Sigma^{T}\) denotes the \(\Sigma\)-product sigma algebra on \(S^{T}\). We call \(S, \Sigma\) the \textbf{state space}.
\end{definition}

Normally, however, we think of a stochastic process as some kind random object that progresses through "time". We could alternately have defined it as an indexed collection of random elements \(\{X_{t}: \Omega, \mathcal{B} \to S, \Sigma\}_{t \in T}\). The definitions are equivalent due to the following proposition

\begin{proposition}[\cite{folland_2011}]
    Let \((Z, \mathcal{M})\), $(Y_{\alpha}, \mathcal{N}_\alpha)$ be measure spaces for $\alpha \in A$. Let $Y = \prod\limits_{\alpha \in A} Y_\alpha$ and let $\mathcal{N} = \bigotimes\limits_{\alpha \in A} \mathcal{N}_\alpha$. Let $f : Z \to Y$ and denote $f_{\alpha} := \pi_{\alpha} \circ f$, where $\pi_\alpha$ is the projection map for $\alpha \in A$. Then $f$ is $(\mathcal{M}, \mathcal{N})$-measureable iff $f_\alpha$ is $(\mathcal{M}, \mathcal{N}_\alpha)$-measureable for every $\alpha \in A$.
\end{proposition}

Therefore, we will use the two definitions interchangeably.

The total ordering in the definition of stochastic process gives us our concept of "time". Usually, \(T = \mathbb{N}\) or \(\mathbb{R}\). However, we can imagine more exotic stochastic processes by choosing a total ordering that cannot be order embedded into \(\mathbb{R}\). For example, we could consider \(\mathbb{R}^{2}, \preccurlyeq\), endowed with \(\preccurlyeq\) is the lexicographic ordering of \(\mathbb{R}^{2}\). Observe that 
\[\{\{t\} \times \mathbb{R} \subset \mathbb{R}^{2} | t \in \mathbb{R}\}\]
is a partition of \(\mathbb{R}^{2}\) into uncountably many nondegenerate \(\preccurlyeq\)-intervals. Thus, if \(\mathbb{R}^{2}, \preccurlyeq\) could be order embedded into \(\mathbb{R}\), then \(\mathbb{R}\) would uncountably many disjoint nondegenerate intervals, each of which must contain a rational. But then the rationals would be uncountable, giving us a contradiction \cite{exotic_order}.

A stochastic process defined with this more exotic total ordering would look like a grid of random variables, with each vertical slice representing a stochastic process over \(\mathbb{R}\). We could instead choose to think of it as a stochastic process of stochastic processes indexed by \(\mathbb{R}\).

A stochastic process on its own does not have much more structure than a random element. We would like to incorporate the idea of measureability with respect to some growing information.

\begin{definition}
    A \textbf{filtration} is an non-decreasing collection of sub-sigma algebras \(\{\mathcal{F}_{t} \subset \mathcal{B}\}_{t \in T}\), indexed by our total ordering \(T\).
\end{definition}

\begin{definition}
    An \textbf{adapted process} with respect to filtration \(\{\mathcal{F}_{t}\}_{t \in T}\) is a stochastic process \(\{X_{t}\}_{t \in T}\) so that \(\forall t \in T\), \(X_{t} \in \mathcal{F}_{t}\).
\end{definition}

Finally, we are ready to give our definition of a martingale, our concept of a fair game.

\begin{definition}
    \label{martingale_defn}
    Let $X: \Omega \to \mathbb{R}^{T}$ be an adapted process wrt filtration $\mathcal{F}_{*} = (\mathcal{F}_{t})_{t \in T}$. $X$ is a \textbf{martingale} if it satisfies the following properties:
    \begin{enumerate}
        \item $X_{t} \in L^{1}(\mathcal{B})$ $\forall t \in T$,
        \item $\mathbb{E}(X_{t} | \mathcal{F}_{r}) = X_{r}$ (\(\mathbb{P}\)-a.s.) for any $r \leq t$.
    \end{enumerate}
\end{definition}

Note that for condition (2) of definition \ref{martingale_defn}, we need only check that it holds for \(r < t\), since it already holds for \(r = t\) by the assumption that \(X\) is an adapted process and by the known information property of proposition \ref{cexpe_properties}.

We can also define the related sub(super)-martingale by changing the equality in condition (2) of definition \ref{martingale_defn} to a \(\geq\) (\(\leq\)). A submartingale represents a favorable game and a supermartingale represents an unfavorable game. We could have alternatively defined submartingale and supermartingale first, then defined a martingale as an adapted process that is both a submartingale and a supermartingale. We will later prove theorems about submartingales, but they will automatically carry over to martingales. To get the corresponding property for supermartingales, we can just look at the supermartingale's negation to turn it into a submartingale.

Note that by applying \textit{smoothing} and the identity \(\mathbb{E}(Y) = \mathbb{E}(Y|\{\emptyset, \Omega\})\), we get that for any \(r \leq t \in T\), 
\[\mathbb{E}(X_{t}) = \mathbb{E}(X_{t} | \{\emptyset, \Omega\}) = \mathbb{E}(\mathbb{E}(X_{t} | \mathcal{F}_{r})|\{\emptyset, \Omega\}) = \mathbb{E}(X_{r}|\{\emptyset, \Omega\}) = \mathbb{E}(X_{r})\]

When our total ordering \(T = \mathbb{N}\), we have the following alternative characterization of a martingale
\begin{proposition}[\cite{resnick_2014,durrett_2010}]
    \label{natural_martingale}
    Let $X: \Omega \to \mathbb{R}^{\mathbb{N}}$ be an adapted process wrt filtration $\mathcal{F}_{*} = (\mathcal{F}_{n})_{n \in \mathbb{N}}$. $X$ is a martingale if and only if it satisfies the following properties:
    \begin{enumerate}
        \item $X_{n} \in L^{1}(\mathcal{B})$ $\forall n \in \mathbb{N}$,
        \item $\mathbb{E}(X_{n+1} | \mathcal{F}_{n}) = X_{n}$ (\(\mathbb{P}\)-a.s.) for any $n \in \mathbb{N}$.
    \end{enumerate}
\end{proposition}

\begin{proof}
    \((\Rightarrow)\) Condition (2) of \ref{natural_martingale} is just a special case of condition (2) of \ref{martingale_defn} when we set \(t = n+1 \geq n = r\).

    \((\Leftarrow)\): This follows by induction. First observe that for $n = 1$, the only $m \in \mathbb{N}$ s.t. $m \leq n$ is $m = 1$. Therefore, $\forall m \leq n$, we have that $\mathbb{E}(X_{n} | \mathcal{F}_{m}) = \mathbb{E}(X_{1} | \mathcal{F}_{1}) = X_{1}$. For the inductive step, suppose this claim holds for $n \in \mathbb{N}$. Consider $m \leq n+1$. If \(m = n+1\), then \(\mathbb{E}(X_{n+1} | \mathcal{F}_{m}) = \mathbb{E}(X_{n+1} | \mathcal{F}_{n+1}) = X_{n+1}\), so we're good. On the other hand, if \(m < n+1\), then we have that $$\mathbb{E}(X_{n+1} | \mathcal{F}_{m}) = \mathbb{E}(\mathbb{E}(X_{n+1} | \mathcal{F}_{n}) | \mathcal{F}_{m}) = \mathbb{E}(X_{n} | \mathcal{F}_{m}) = X_{m}$$ by the \textit{smoothing} property of proposition \ref{cexpe_properties} and induction.
\end{proof}

We refer to such martingales as \textbf{discrete-time} martingales.

% TODO:
% - \st{definition of stochastic process}
% - \st{remark about total ordering}
% - \st{definition of adapted process and filter}
% - \st{definition of martingale (total ordering), super, sub}
% - \st{connection to natural number definition}

\section{Decomposition of Discrete-Time martingales}

A useful concept for discrete-time martingales is a predictable process

\begin{definition}[\cite{resnick_2014,durrett_2010}]
    Let $X: \Omega \to S^{\mathbb{N}}$ be an adapted process wrt filtration $(\mathcal{F}_{n})_{n \in \mathbb{N}}$ and state space \(S, \Sigma\). \(X\) is a \textbf{predictable process} if for all \(n \in \mathbb{N}\), \(X_{n} \in \mathcal{F}_{n-1}\), with \(\mathcal{F}_{0} = \{\emptyset, \Omega\}\).
\end{definition}

This concept is capturing the idea that we can "predict" the next value of a stochastic process \(X\) using the information we have now. Indeed, for a real-valued predictable process, if at timestep \(n\) we can discern between whether a given event in \(\mathcal{F}_{n}\) occured, then for any \(x \in \mathbb{R}\) we can determine whether or not \(X_{n+1} = x\) since \([X_{n+1}=x] \in \mathcal{F}_{n}\). By setting \(\mathcal{F}_{0} = \{\emptyset, \Omega\}\), we enforce that \(X_{1}\) is a constant.

The following proposition reveals that we can think of any adapted process indexed by \(\mathbb{N}\) as a predictable process with some "fair" noise.

\begin{proposition}
    \label{process_decomposition}
    Let $X: \Omega \to \mathbb{R}^{\mathbb{N}}$ be a discrete-time adapted process wrt filtration $\mathcal{F}_{*} := (\mathcal{F}_{n})_{n \in \mathbb{N}}$ so that \(X_{n} \in L^{1}(\mathcal{B})\) for all \(n \geq 1\). Then there exists a martingale \((M_{n})_{n \geq 1}\) and a predictable process \((A_{n})_{n \geq 1}\) with \(A_{1} = 0\) so that for \(n \geq 1\)
    \[X_{n} = M_{n} + A_{n}.\]
    This decomposition is \(\mathbb{P}\)-a.s. unique.
\end{proposition}

Before we proceed with the proof of proposition \ref{process_decomposition}, we first establish a useful concept and lemma.

\begin{definition}[\cite{resnick_2014}]
    \label{difference_defn}
    Let $d: \Omega \to \mathbb{R}^{\mathbb{N}}$ be a discrete-time process adapted to filtration $(\mathcal{F}_{n})_{n \in \mathbb{N}}$. Then, it is a \textbf{martingale difference sequence} if
    \begin{enumerate}
        \item $d_{j} \in L^{1}(\mathcal{B})$ $\forall j \in \mathbb{N}$
        \item $\mathbb{E}(d_{j+1} | \mathcal{B}_{j}) = 0$ \(\forall j \in \mathbb{N}\)
    \end{enumerate}
\end{definition}

\begin{lemma}[\cite{resnick_2014}]
    \label{difference_martingale_equiv}
    \((M_{n})_{n \geq 1}\) is a martingale adapted to filtration \(\mathcal{F}_{*} = (\mathcal{F}_{n})_{n \geq 1}\) iff there exists a martingale difference sequence $d: \Omega \to \mathbb{R}^{\mathbb{N}}$  adapted to the same filtration and \(c \in \mathbb{R}\) so that
    \[M_{n} := c + \sum\limits_{j=1}^{n} d_{j}\]
    for all \(n \in \mathbb{N}\).
\end{lemma}

\begin{proof}
    \((\Rightarrow)\): Let \(d_{n} := M_{n} - M_{n-1}\) for \(n \geq 1\), where \(M_{0} = \mathbb{E}(M_{1})\). Then for \(n \geq 1\), the sum \(M_{0} + \sum\limits_{j=1}^{n} d_{j} = M_{0} + \sum\limits_{j=1}^{n} M_{j} - M_{j-1}\) is telescoping and is equal to \(M_{n}\).
    
    Now we need simply show that \((d_{n})_{n \geq 1}\) is a martingale difference sequence. First note that \((d_{n})_{n \geq 1}\) is adapted to \(\mathcal{F}_{*}\) since \(M_{n} - M_{n-1} \in \mathcal{F}_{n}\) for all \(n \geq 1\). Checking condition (1) of \ref{difference_defn}, we see for \(n \geq 1\) that \(\mathbb{E}|d_{n}| = \mathbb{E}|M_{n} - M_{n-1}| \leq \mathbb{E}|M_{n}| + \mathbb{E}|M_{n-1}|< \infty\), so \(d_{n} \in L^{1}(\mathcal{B})\).

    Checking condition (2), we see that
    \[\mathbb{E}(d_{n+1}|\mathcal{F}_{n}) = \mathbb{E}(M_{n+1}|\mathcal{F}_{n})- \mathbb{E}(M_{n}|\mathcal{F}_{n})) = M_{n} - M_{n} = 0. \]

    Both conditions are satisfied so \((d_{n})_{n \geq 1}\) is a martingale difference sequence. \(\checkmark\)

    \((\Leftarrow)\): We are given difference sequence \((d_{n})_{n \geq 1}\) and constant \(c \in  \mathbb{R}\). Note that since \((d_{n})_{n \geq 1}\) is adapted to \(\mathcal{F}_{*}\), we know \(M_{n} = c + \sum\limits_{j=1}^{n} d_{j} \in \mathcal{F}_{n}\) for all \(n \geq 0\), so \((M_{n})_{n \geq 1}\) is adapted to \(\mathcal{F}_{*}\). Now, checking for \(n \in \mathbb{N}\) that \(M_{n} \in L^{1}(\mathcal{B})\), consider
    \begin{align*}
        \mathbb{E}|M_{n}| &= \mathbb{E}\left| c + \sum\limits_{j=1}^{n} d_{j}  \right| \\
        & \leq  |c| + \sum\limits_{j=1}^{n} \mathbb{E}| d_{j} | \\
        & < \infty
    \end{align*}
    since \(d_{j} \in L^{1}(\mathcal{B})\) \(\forall j \in \mathbb{N}\).

    Next we check condition (2) of definition \ref{martingale_defn}. Letting \(n \in \mathbb{N}\)
    \begin{align*}
        \mathbb{E}(M_{n+1}|\mathcal{F}_{n}) &= c + \sum\limits_{j=1}^{n+1} \mathbb{E}(d_{j}|\mathcal{F}_{n}) \\
        &= \mathbb{E}(d_{n+1}|\mathcal{F}_{n}) +  c + \sum\limits_{j=1}^{n} d_{j} \\
        &=c + \sum\limits_{j=1}^{n} d_{j} & ((d_{n})_{n \geq 1} \text{is a difference sequence})\\
        &= M_{n}.
    \end{align*}

    Both conditions are satisfied, so \((M_{n})_{n \geq 1}\) is a martingale. \(\checkmark\)
\end{proof}

Note that our proof above also gives a procedure to construct a martingale difference sequence from a martingale. Now we proceed to the proof of proposition \ref{process_decomposition}
\begin{proof}[Proof of Proposition \ref{process_decomposition}]
    We will deconstruct \((X_{n})_{n \geq 1}\) into
    \begin{align*}
        M_{n} &:= X_{0} + \sum\limits_{j=1}^{n} X_{j} - \mathbb{E}(X_{j}|\mathcal{F}_{j-1}) \\
        A_{n} &:=  \sum\limits_{j=1}^{n} \mathbb{E}(X_{j}|\mathcal{F}_{j-1}) - X_{j-1} \\
    \end{align*}
    where we let \(\mathcal{F}_{0} := \{\emptyset, \Omega\}\) and \(X_{0} = \mathbb{E}(X_{1}|\mathcal{F}_{0}) = \mathbb{E}(X_{1})\). Observe that \(A_{1} = \mathbb{E}(X_{1}|\mathcal{F}_{0}) - X_{0} = \mathbb{E}(X_{1}) - \mathbb{E}(X_{1}) = 0\). With this choice of \((M_{n})_{n \geq 1}\) and \((A_{n})_{n \geq 1}\) we have that
    \begin{align*}
        M_{n} + A_{n} &= X_{0} + \sum\limits_{j=1}^{n} X_{j} - \mathbb{E}(X_{j}|\mathcal{F}_{j-1}) + \sum\limits_{j=1}^{n} \mathbb{E}(X_{j}|\mathcal{F}_{j-1}) - X_{j-1} \\
        &=  X_{0} + \sum\limits_{j=1}^{n} X_{j} - \mathbb{E}(X_{j}|\mathcal{F}_{j-1}) + \mathbb{E}(X_{j}|\mathcal{F}_{j-1}) - X_{j-1} \\
        &= X_{0} + \sum\limits_{j=1}^{n} X_{j} - X_{j-1} \\
        &= X_{n}.
    \end{align*}
    Next we show that \((M_{n})_{n \geq 1}\) is a martingale and \((A_{n})_{n \geq 1}\) is a predictable sequence. To that end, observe that for \(n \geq 1\):
    \begin{enumerate}
        \item \(X_{n} - \mathbb{E}(X_{n}|\mathcal{F}_{n-1}) \in \mathcal{F}_{n}\) since \((X_{n})_{n \geq 1}\) is an adapted process and \(\mathcal{F}_{n-1} \subset \mathcal{F}_{n}\).
        \item We are given that \(X_{n} \in L^{1}(\mathcal{B})\) and \(\mathbb{E}(X_{n}|\mathcal{F}_{n-1})\in L^{1}(\mathcal{B})\) by definition, so \(X_{n} - \mathbb{E}(X_{n}|\mathcal{F}_{n-1}) \in L^{1}(\mathcal{B})\).
        \item \(\mathbb{E}(X_{n} - \mathbb{E}(X_{n}|\mathcal{F}_{n-1})|\mathcal{F}_{n-1})= \mathbb{E}(X_{n}|\mathcal{F}_{n-1}) - \mathbb{E}(X_{n}|\mathcal{F}_{n-1}) = 0\)
    \end{enumerate}
    so \(X_{n} - \mathbb{E}(X_{n}|\mathcal{F}_{n-1})\) is a martingale difference sequence. By lemma \ref{difference_martingale_equiv}, noting that \(X_{0} = \mathbb{E}(X_{1})\) is a constant, we have that \( M_{n} = X_{0} + \sum\limits_{j=1}^{n} X_{j} - \mathbb{E}(X_{j}|\mathcal{F}_{j-1})\) is a martingale.

    On the other hand, for all \(j \geq 1\), we know that \(\mathbb{E}(X_{j}|\mathcal{F}_{j-1}) - X_{j-1} \in \mathcal{F}_{j}\) since \((X_{n})_{n \geq 1}\) is an adapted process. Because \(\mathcal{F}_{*}\) is a filtration, for all \(m \geq j\), \(\mathcal{F}_{j} \subset \mathcal{F}_{m}\). Therefore, for all \(n \geq 1\), \(A_{n} =  \sum\limits_{j=1}^{n} \mathbb{E}(X_{j}|\mathcal{F}_{j-1}) - X_{j-1} \in \mathcal{F}_{n}\) and it is predictable.

    Finally we show that this decomposition is (almost surely) unique. Suppose there existed another decomposition with the same properties: \(X_{n} = N_{n} + B_{n}\). We will apply induction to show that these decompositions are almost surely the same. For our base case, let \(n = 1\). We are given that \(A_{1} = B_{1} = 0\). Therefore, after subtracting the two decompositions, \(0 = M_{1} + A_{1} - N_{1} + B_{1} = M_{1} - N_{1}\), so \(M_{1} = N_{1}\). For our inductive step, assume for some \(n \geq 1\) that \(M_{n} = N_{n}\) and \(A_{n} = B_{n}\) almost surely. We know that \(0 = M_{n+1} + A_{n+1} - N_{n+1} - B_{n+1}\). Taking conditional expectations of both sides conditioned on \(\mathcal{F}_{n}\), we get
    \begin{align*}
        0 &= \mathbb{E}(M_{n+1} + A_{n+1} - N_{n+1} - B_{n+1}|\mathcal{F}_{n}) \\
        &= M_{n} - N_{n} + \mathbb{E}(A_{n+1}|\mathcal{F}_{n}) - \mathbb{E}(B_{n+1}|\mathcal{F}_{n}) & ((M_{n})_{n \geq 1}, (N_{n})_{n \geq 1} \text{ are martingales})\\
        &= M_{n} - N_{n} + A_{n+1} - B_{n+1} & ((A_{n})_{n \geq 1}, (B_{n})_{n \geq 1} \text{ are predictable}) \\
        &= A_{n+1} - B_{n+1} & \text{(inductive hypothesis)} \\
    \end{align*}

    Therefore \(A_{n+1} = B_{n+1}\) almost surely. Then \(0 = M_{n+1} + A_{n+1} - N_{n+1} - B_{n+1} = M_{n+1} - N_{n+1}\), so \(M_{n+1} = N_{n+1}\) almost surely, proving the inductive hypothesis for \(n+1\). Therefore, by induction, we have that  \(M_{n+1} + A_{n+1}\) is the almost surely unique decomposition of \(X_{n}\) for \(n \geq 1\) with the desired properties.
\end{proof}

We can interpret proposition \ref{process_decomposition} as saying any adapted process is some kind of "trend" with "fair" additive noise applied on top. This result naturally gives us some well-known decomposition results.

\begin{corollary}[Doob's Submartingale Decomposition \cite{resnick_2014}]
    \label{doob_decomposition}
    Let \((Y_{n})_{n \geq 1}\) be a discrete-time adapted process wrt filtration \(\mathcal{F}_{*} := (\mathcal{F}_{n})_{n \geq 1}\) so that \(Y_{n} \in L^{1}(\mathcal{B})\) for all \(n \geq 1\). Then it is a submartingale iff there exists a martingale \((M_{n})_{n \geq 1}\) and a non-decreasing predictable process \((A_{n})_{n \geq 1}\) with \(A_{1} = 0\) so that for \(n \geq 1\)
    \[Y_{n} = M_{n} + A_{n}\]
    This decomposition is almost surely unique.
\end{corollary}

\begin{proof}
    \((\Rightarrow)\): Apply proposition \ref{process_decomposition} to get our almost surely unique martingale \((M_{n})_{n \geq 1}\) and predictable process \((A_{n})_{n \geq 1}\) so that \(Y_{n} = M_{n} + A_{n}\) for \(n \geq 1\). We must simply show that \(A_{n}\) is non-decreasing. To that end, observe that for \(n \geq 2\)
    \begin{align*}
        0 &\leq \mathbb{E}(Y_{n}|\mathcal{F}_{n-1}) - Y_{n-1} \\
        &= \mathbb{E}(M_{n} + A_{n}|\mathcal{F}_{n-1}) - A_{n-1} \\
        &= \mathbb{E}(M_{n}|\mathcal{F}_{n-1})  - M_{n-1} + \mathbb{E}(A_{n}|\mathcal{F}_{n-1}) - A_{n-1} \\
        &= M_{n-1} - M_{n-1} + \mathbb{E}(A_{n}|\mathcal{F}_{n-1}) - A_{n-1} &((M_{n})_{n \geq 1} \text{is a martingale}) \\
        &=A_{n}- A_{n-1}, &((A_{n})_{n \geq 1} \text{is a predictable process})
    \end{align*}
    giving us that \((A_{n})_{n \geq 1}\) is non-decreasing. \(\checkmark\)

    \((\Leftarrow)\): We must show that \(Y_{n} = M_{n} + A_{n}\) for \(n \geq 1\) is a submartingale. We only need to check condition (2) since we are given \(Y_{n} \in L^{1}(\mathcal{B})\). Checking, for \(n \geq 1\), we see
    \begin{align*}
        \mathbb{E}(Y_{n+1}|\mathcal{F}_{n}) &= \mathbb{E}(M_{n+1} + A_{n+1}|\mathcal{F}_{n}) \\
        &= \mathbb{E}(M_{n+1}|\mathcal{F}_{n}) + A_{n+1} \\
        &= M_{n} + A_{n+1} & ((M_{n})_{n \geq 1} \text{ is a martingale})\\
        &\geq M_{n} + A_{n} & ((A_{n})_{n \geq 1} \text{ is non-decreasing})\\
        &= Y_{n},
    \end{align*}
    so \((Y_{n})_{n \geq 1}\) is indeed a submartingale. \(\checkmark\)
\end{proof}


% TODO: 
% - \st{predictable process definition}
% - \st{decomp of adapted process into predictable process and martingale}
% - \st{Doob's Decomposition}

\section{Strategies and Discrete-Time martingales}

To further analyze martingales, we introduce the concept of a "strategy" for playing the "fair game", where we our strategy determines our move at the end of each turn.

\begin{definition}[\cite{durrett_2010}]
    \label{strategy}
    Let $X: \Omega \to \mathbb{R}^{\mathbb{N}}$ be a discrete-time adapted process wrt filtration $\mathcal{F}_{*} := (\mathcal{F}_{n})_{n \in \mathbb{N}}$ and let $H: \Omega \to \mathbb{R}^{\mathbb{N}}$ be a predictable process on the same filtration. Then we call \(H\) a \textbf{strategy} for \(X\) with \textbf{winnings} 
    \[(H \cdot X)_{n} := \sum\limits_{m=1}^{n} H_{m} (X_{m} - X_{m-1})\]
    for $n \geq 1$ (where \(X_{0} = 0\)).
\end{definition}

The following proposition will establish that we cannot change the fairness of a game by choosing the "best" (or "worst") strategy.

\begin{proposition}[\cite{durrett_2010}]
    \label{fairness_preservation}
    Let $(Y_{n})_{n \geq 1}$ be a discrete-time martingale wrt filtration $\mathcal{F}_{*} := (\mathcal{F}_{n})_{n \in \mathbb{N}}$ and let \((H_{n})_{n \geq 1}\) be bounded strategy for \((Y_{n})_{n \geq 1}\). Then \(((H \cdot Y)_{n})_{n \geq 1}\) is a martingale.
\end{proposition}
\begin{proof}
    First note that since \(Y_{0}, Y_{1}, \dots, Y_{n}, H_{1}, \dots, H_{n} \in \mathcal{F}_{n}\), we have that \((H \cdot Y)_{n} \in \mathcal{F}_{n}\), so \((H \cdot Y)_{n}\) is adapted to \(\mathcal{F}_{*}\).
    
    Next note that for $n \geq 1$ because $H_{n}$ is bounded, say by $M \in \mathbb{R}_{\geq 0}$ 
    \begin{align*}
        \mathbb{E}|(H \cdot Y)_{n}| &= \mathbb{E}\left|\sum\limits_{m=2}^{n}H_{m}(Y_{m} - Y_{m-1})\right|\\
        &\leq M \sum\limits_{m=1}^{n} \mathbb{E}|Y_{m} - Y_{m-1}| & \text{(Triangle Inequality)}\\
        &\leq M \sum\limits_{m=1}^{n} \mathbb{E}|Y_{m}| + \mathbb{E}| Y_{m-1}| & \text{(Triangle Inequality)}\\
        &< \infty
    \end{align*}
    since $Y_{m} \in L^{1}(\mathcal{B})$ $\forall m \geq 0$. So $(H \cdot Y)_{n} \in L^{1}(\mathcal{B})$ as well. $\checkmark$

    Observe that for $n \geq 1$
    \begin{align*}
        \mathbb{E}((H \cdot Y)_{n+1} | \mathcal{F}_{n}) &= \mathbb{E}(\sum\limits_{m=1}^{n+1} H_{m}(Y_{m} - Y_{m-1}) | \mathcal{F}_{n})\\
        &=\sum\limits_{m=1}^{n+1}\mathbb{E}(H_{m}(Y_{m} - Y_{m-1}) | \mathcal{F}_{n})\\
        &=\sum\limits_{m=1}^{n}H_{m}(Y_{m} - Y_{m-1}) + \mathbb{E}(H_{n+1} (Y_{n+1} - Y_{n}) | \mathcal{F}_{n})\\
        &= (H \cdot Y)_{n}
    \end{align*}
    since \((Y_{n})_{n \geq 1}\) is a martingale, so
    \begin{align*}
        \mathbb{E}(H_{n+1} (Y_{n+1} - Y_{n}) | \mathcal{F}_{n}) &= H_{n+1} (\mathbb{E}(Y_{n+1} | \mathcal{F}_{n}) -Y_{n})\\
        &= 0. \checkmark
    \end{align*}
     
    Therefore, we have that \(((H \cdot Y)_{n})_{n \geq 1}\) is a martingale.
\end{proof}

Note that the boundedness of our strategy was only used to ensure the winnings process is integrable. We could replace the boundedness condition with alternate conditions. For example, if we had $H_{n}, Y_{n} \in L^{2}(\mathcal{B})$ $\forall n \in \mathbb{N}$, then we could apply the cauchy-schwarz inequality to get that $(H \cdot Y)_{n} \in L^{1}(\mathcal{B})$.

Using the Doob decomposition, we can easily generalize this result to submartingales (and therefore supermartingales by negation):

\begin{corollary}
    \label{favorability_preservation}
    Let $(Y_{n})_{n \geq 1}$ be a discrete-time submartingale wrt filtration $\mathcal{F}_{*} := (\mathcal{F}_{n})_{n \in \mathbb{N}}$ and let \((H_{n})_{n \geq 1}\) be non-negative bounded strategy for \((Y_{n})_{n \geq 1}\). Then \(((H \cdot Y)_{n})_{n \geq 1}\) is a submartingale.
\end{corollary}
    
\begin{proof}
    Using corollary \ref{doob_decomposition}, decompose \((Y_{n})_{n \geq 1}\) into \((M_{n} + A_{n})_{n \geq 1}\), where \((M_{n})_{n \geq 1}\) is a martingale and \((A_{n})_{n \geq 1}\) is a predictable process with \(A_{1} = 0\). Denote \(M_{0} = A_{0} = 0\) to make \(Y_{0} = M_{0} + A_{0} = 0\). Then,
    \begin{align*}
        (H \cdot Y)_{n} &= \sum\limits_{m=1}^{n}H_{m}(Y_{m} - Y_{m-1}) \\
        &= \sum\limits_{m=1}^{n}H_{m}(M_{m} + A_{m} - M_{m-1} - A_{m-1}) \\
        &= \sum\limits_{m=1}^{n}H_{m}(M_{m} - M_{m-1}) + \sum\limits_{m=1}^{n}H_{m}( A_{m}  - A_{m-1}) \\
        &= (H \cdot M)_{n} + (H \cdot A)_{n}
    \end{align*}
    We know by proposition \ref{fairness_preservation} that \(((H \cdot M)_{n})_{n \geq 1}\) is a martingale. Furthermore, we have that 
    \begin{enumerate}
        \item \(((H \cdot A)_{n})_{n \geq 1}\) is a predictable process because  \((H_{n})_{n \geq 1}\) and \((A_{n})_{n \geq 1}\) are predictable processes.
        \item \((H \cdot A)_{1} = H_{1} A_{1} = 0\).
        \item For all \(n \geq 1\), \((H \cdot A)_{n+1} = (H \cdot A)_{n} + H_{n+1} (A_{n+1} - A_{n}) \geq (H \cdot A)_{n}\) because \(H_{n+1} \geq 0\) and \(A_{n+1} \geq A_{n}\) (it is non-decreasing). Thus \(((H \cdot A)_{n})_{n \geq 1}\) is a non-decreasing sequence. 
    \end{enumerate}

    Therefore, by the converse case of corollary \ref{doob_decomposition}, we have that  \(((H \cdot Y)_{n})_{n \geq 1} = ( (H \cdot M)_{n} + (H \cdot A)_{n})_{n \geq 1}\) is a submartingale.
\end{proof}

Corresponding results hold for submartingales and martingales. If $(Y_{n})_{n \geq 1}$ is a martingale, then bounded \((H_{n})_{n \geq 1}\) will suffice, it does not need to be non-negative. 

We can look at playing the game up to a stopping point as a special kind of strategy. The classical definition of a "stopping time" is as follows

\begin{definition}[\cite{durrett_2010,resnick_2014}]
    Let $\mathcal{F}_{*} := (\mathcal{F}_{n})_{n \in \mathbb{N}}$ be a filtration. A stopping time on \(\mathcal{F}_{*}\) is a random variable \(\nu: \Omega \to \bar{\mathbb{N}}\) so that for each \(n \in \mathbb{N}\), \([\nu = n] \in \mathcal{F}_{n}\). Here, \(\bar{\mathbb{N}}\) is the extended natural numbers.
\end{definition}

There are a couple equivalent conditions for being a stopping time. 
\begin{proposition}[\cite{resnick_2014}]
    \label{stopping_time_equiv_conds}
    Suppose $\mathcal{F}_{*} := (\mathcal{F}_{n})_{n \in \mathbb{N}}$ is a filtration on $\Omega$. Let $\nu : \Omega \to \bar{\mathbb{N}}$. The following conditions are equivalent:
    \begin{enumerate}
        \item $\nu$ is a stopping time.
        \item $[\nu \leq n] \in \mathcal{F}_{n}, \forall n \in \mathbb{N}$
        \item $[\nu > n] \in \mathcal{F}_{n}, \forall n \in \mathbb{N}$
    \end{enumerate}
\end{proposition}

\begin{proof}
    $(1) \Rightarrow (2)$: We have for $n \in \mathbb{N}$ 
    $$[\nu \leq n] = \bigcup\limits_{m \leq n}[\nu =m] \in \mathcal{F}_{n}$$
    since $[\nu = m] \in \mathcal{F}_{m} \subset \mathcal{F}_{n}$ $\forall m \leq n$. $\checkmark$

    $(2) \Rightarrow (3)$: Let $n \in \mathbb{N}$. Since $[\nu \leq n] \in \mathcal{F}_{n}$, we know by definition of a sigma-algebra that $[\nu > n] = [\nu \leq n]^{C} \in \mathcal{F}_{n}$. $\checkmark$

    $(3) \Rightarrow (1)$: Let $n \in \mathbb{N}$. Since $[\nu > n] \in \mathcal{F}_{n}$ and because $\mathcal{F}_{n}$ forms a filtration, we have that
    $$[\nu = n] =  [\nu > n-1] \cap ([\nu > n])^{C} \in \mathcal{F}_{n},$$
    where $[\nu > 0] = \Omega$. $\checkmark$
\end{proof}

We can alternatively view a stopping time as a special kind of strategy:

\begin{proposition}[\cite{durrett_2010}]
    Suppose $\mathcal{F}_{*} := (\mathcal{F}_{n})_{n \in \mathbb{N}}$ is a filtration on $\Omega$. Let $\nu : \Omega \to \bar{\mathbb{N}}$. If \(\nu\) is a stopping time, then \((1_{\nu \geq n})_{n \geq 1}\) is a predictable process.
\end{proposition}

\begin{proof}
    We need only check that \(1_{\nu \geq n} \in \mathcal{F}_{n-1}\) for \(n \geq 1\) (with \(\mathcal{F}_{0} = \{\emptyset, \Omega\}\)). To that end we have \(1_{\nu \geq 0} = 1\) since \(\nu(\omega) \in \bar{\mathbb{N}}\) for all \(\omega \in \Omega\), so \(1_{\nu \geq 0} \in \mathcal{F}_{0}\). For \(n \geq 1\), we have that \(1_{\nu \geq n} = 1 - 1_{\nu \leq n-1} \in \mathcal{F}_{n-1}\) since \([\nu \leq n-1] \in \mathcal{F}_{n-1}\) by proposition \ref{stopping_time_equiv_conds}.
\end{proof}

For a adapted process $X: \Omega \to \mathbb{R}^{\mathbb{N}}$, we can denote the process resulting from stopping the game after \(\nu\) turns as:
\[X_{n \wedge \nu} := (1_{\nu \geq \cdot} \cdot X)_{n} = \sum\limits_{m=1}^{n} 1_{\nu \geq n} (X_{m} - X_{m-1}).\]

Combining with our result about strategies, we get the following result:

\begin{corollary}
    Let $(X_{n})_{n \geq 1}$ be a discrete-time (sub/super)martingale wrt filtration $\mathcal{F}_{*} := (\mathcal{F}_{n})_{n \in \mathbb{N}}$ and let \(\nu\) be stopping time on \(\mathcal{F}_{*}\). Then \((X_{n \wedge \nu})_{n \geq 1}\) is a (sub/super)martingale.
\end{corollary}

\begin{proof}
    This follows from the definition of \(X_{n \wedge \nu}\) and application of proposition \ref{fairness_preservation} or corollary \ref{favorability_preservation}.
\end{proof}

% TODO:
% - \st{Definition}
% - \st{preservation of fairness}
% - \st{relationship to stopping times}

\section{Convergence of Discrete-time Martingales}

So far we have concerned ourselves with properties of martingales without looking at any limiting behavior. Under the right conditions, we will find that discrete-time martingales do converge to a random variable. To establish this, we'll take approach of analyzing upcrossings.

\begin{proposition}[\cite{resnick_2014}, \cite{durrett_2010}]
    Let $(\mathcal{F}_{n})_{n=1}^{\infty}$ be a filtration and let $(X_{n})_{n=1}^{\infty}$ be adapted to $(\mathcal{F}_{n})_{n=1}^{\infty}$. Let $a < b \in \mathbb{R}$. Then the following random variables are stopping times for $k \geq 1$:
    \begin{align*}
        N_{2k-1} &:= \inf\limits \{m > N_{2k-2} : X_{m} \leq a\}\\
        N_{2k} &:= \inf\limits \{m > N_{2k-1} : X_{m} \geq b\}\\
    \end{align*}
    with \(N_{0} = 0\). We define the $\mathcal{F}_{n}$-random variable 
    $$U_{n}[a,b] = \sup\limits \{k \geq 0 : N_{2k} \leq n\}$$
    to be the number of \textbf{complete upcrossings} by time $n \in \mathbb{N}$.
\end{proposition}

\begin{proof}
    We wish to check that \(N_{2k-1}, N_{2k}\) are stopping times for \(k \geq 1\) and that \(U_{n}[a,b] \in \mathcal{F}_{n}\). We establish the first claim by induction. First observe that, letting $n \in \mathbb{N}$,
    \begin{align*}
        [N_{1} = n] &= [X_{1} > a, \dots, X_{n-1} > a] \cap [X_{n} \leq a] \in \mathcal{F}_{n}\\
    \end{align*}
    by virtue of $(\mathcal{F}_{n})_{n=1}^{\infty}$ being a filtration, so $N_{1}$ is a stopping time. Now suppose $N_{1}, \dots, N_{2k-1}$ are stopping times for $k \geq 1$ . Then, letting $n \in \mathbb{N}$,
    \begin{align*}
        [N_{2k} = n] &= [X_{N_{2k-1}+1} < b, \dots, X_{n-1} < b] \cap [X_{n} \geq b] \in \mathcal{F}_{n}\\
        &\bigcup\limits_{m = 1}^{n-1} [X_{m+1} < b, \dots, X_{n-1} < b] \cap [X_{n} \geq b] \cap [N_{2k-1} = m] \in \mathcal{F}_{n}
    \end{align*}
    by virtue of $N_{2k-1}$ being a stopping time and $(\mathcal{F}_{n})_{n=1}^{\infty}$ being a filtration. Thus, $N_{2k}$ is a stopping time. A similar approach will show that $N_{2k+1}$ must also be a stopping time.
    
    Now we check that $U_{n}[a,b] \in \mathcal{F}_{n}$. Let $k \in \mathbb{N}$, then 
    $$[U_{n}[a,b] = k] = [N_{2k} \leq n] \cap [N_{2k+2} > n] \in \mathcal{F}_{n},$$
    which holds because $N_{2k}$ and $N_{2k+2}$ are both stopping times.
\end{proof}

\begin{proposition}[\cite{durrett_2010}]
    Let $X: \Omega \to \mathbb{R}^{\mathbb{N}}$ be discrete-time process adapted to $\mathcal{F}_{*} := (\mathcal{F}_{n})_{n \in \mathbb{N}}$. Let
    $$H_{n} := \begin{cases} 
    1 & \text{if } \exists k \geq 1 \text{ s.t. } N_{2k-1} < n \leq N_{2k} \\
    0  & \text{otherwise}
    \end{cases}$$
    Then $(H_{n})_{n \geq 1}$ is a predictable process, and thus a strategy for $(X_{n})_{n \geq 1}$. We call it the \textbf{upcrossing strategy}.
\end{proposition}

\begin{proof}
    $$H_{n} = \sum\limits_{k=1}^{\infty} 1_{N_{2k-1} \leq n-1} 1_{n-1 < N_{2k}}$$
    and $N_{j}$ are all stopping times for $j \geq 1$. Therefore $H_{n} \in \mathcal{F}_{n-1}$ and it is predictable.
\end{proof}

We now prove some sufficient criteria for a submartingale (and thus a martingale) to converge almost surely to a random variable in \(L^{1}(\mathcal{B})\). First a useful lemma about submartingales

\begin{lemma}[\cite{resnick_2014}]
    Let $(X_{n})_{n \geq 1}$ be a discrete-time submartingale wrt filtration $\mathcal{F}_{*} := (\mathcal{F}_{n})_{n \in \mathbb{N}}$. Let \(\psi: \mathbb{R} \to \mathbb{R}\) be a non-decreasing convex function so that \(\psi(X_{n}) \in L^{1}(\mathcal{B})\) for all \(n \geq 1\). Then $(\psi(X_{n}))_{n \geq 1}$ is a submartingale. 
\end{lemma}

\begin{proof}
    Since all real convex functions are continuous, we have that \(\psi(X_{n}) \in \mathcal{F}_{n}\) since \(X_{n} \in \mathcal{F}_{n}\) and \(\psi\) is borel measureable for all \(n \geq 1\). We have already assumed that \(\psi(X_{n}) \in L^{1}(\mathcal{B})\) for all \(n \geq 1\). Finally, letting \(n \geq 1\), observe that
    \[\mathbb{E}(\psi(X_{n+1})|\mathcal{F}_{n}) \geq \psi(\mathbb{E}(X_{n+1}|\mathcal{F}_{n})) \geq \psi(X_{n}).\]
\end{proof}

\begin{proposition}[Upcrossing Inequality \cite{resnick_2014,durrett_2010}]
    \label{upcrossing_inequality}
    Let $(X_{n})_{n \geq 1}$ be a discrete-time submartingale wrt filtration $\mathcal{F}_{*} := (\mathcal{F}_{n})_{n \in \mathbb{N}}$. Then $\forall a < b \in \mathbb{R}$, 
    $$(b-a)\mathbb{E}(U_{n}[a,b]) \leq \mathbb{E}(X_{n} - a)^{+} - \mathbb{E}(X_{1} - a)^{+}$$
\end{proposition}

\begin{proof}
    Let $(H_{n})_{n \geq 1}$ be the upcrossing strategy for $(X_{n})_{n \geq 1}$. We let $Y_{n} = a + (X_{n} - a)^{+}$ for $n \geq 1$. Because of the convexity of $(\cdot)^{+}$, we have that $(Y_{n})_{n \geq 1}$ is a submartingale. The only difference between $X_{n}$ and $Y_{n}$ is that $Y_{n} = a$ when $X_{n} \leq a$ for all $n \geq 1$. Therefore $(Y_{n})_{n \geq 1}$ has a the same number of complete upcrossings as $(X_{n})_{n \in \mathbb{N}}$. Since $(Y_{n})_{n \geq 1}$ cannot dip below $a$, we have that
    \begin{equation}
        \label{upcrossing_eqn}
        (b-a)U_{n}[a,b] \leq (H \cdot Y)_{n} = \sum\limits_{m=1}^{n} H_{m}(Y_{m} - Y_{m-1})
    \end{equation}
    with $Y_{0} = 0$ as in the definition \ref{strategy}, since $H_{n} = 1$ if and only if $Y_{n}$ is rising up to $b$ after a fall from above $b$ down to $a$ for $n \geq 2$. That is, we will include a $(b-a)$ for each upcrossing in the sum along with any extra that comes from breaking above $b$ and any remaining incomplete upcrossing. Furthermore, for $n \geq 1$
    $$Y_{n} = \sum\limits_{m=1}^{n} Y_{m} - Y_{m-1} = \sum\limits_{m=1}^{n} (H_{m} - (1 - H_{m}))(Y_{m} - Y_{m-1}) = (H \cdot Y)_{n} + ((1-H) \cdot Y)_{n}.$$
    Denote $K := 1-H$. Thus, $\mathbb{E}(Y_{n})= \mathbb{E}((H \cdot Y)_{n}) + \mathbb{E}((K \cdot Y)_{n})$. Since $K$ is just another strategy, corollary \ref{favorability_preservation} tells us that $((K \cdot Y)_{n})_{n \geq 1}$ is another submartingale. Then, we have that $\mathbb{E}((K \cdot Y)_{n}) \geq \mathbb{E}((K \cdot Y)_{1})$ by smoothing. Note that $H_{1} = 0$ since all $N_{2k-1} \geq 1$ for $k \geq 1$. Thus 
    $$(K \cdot Y)_{1} = Y_{1} - Y_{0} = Y_{1}.$$
    Therefore, $\mathbb{E}(Y_{n}) - \mathbb{E}(Y_{1}) \geq \mathbb{E}(H \cdot Y)_{n}$. Putting it together with \ref{upcrossing_eqn}, we get
    $$(b-a)\mathbb{E}(U_{n}[a,b]) \leq \mathbb{E}((H \cdot Y)_{n}) \leq \mathbb{E}(Y_{n}) - \mathbb{E}(Y_{1})$$
    Substituting our original expression for $Y_{n}$,
    $$(b-a)\mathbb{E}(U_{n}[a,b]) \leq a + \mathbb{E}(X_{n} - a)^{+} - a - \mathbb{E}(X_{1} - a)^{+} = \mathbb{E}(X_{n} - a)^{+} - \mathbb{E}(X_{1} - a)^{+}$$
\end{proof}

Using proposition \ref{upcrossing_inequality}, we can prove convergence submartingales with uniformly bounded first moment:

\begin{theorem}[Martingale Convergence Theorem \cite{durrett_2010,resnick_2014}]
    \label{martingale_convergence}
    Let $(X_{n})_{n \geq 1}$ be a discrete-time submartingale wrt filtration $\mathcal{F}_{*} := (\mathcal{F}_{n})_{n \in \mathbb{N}}$ so that \(\sup\limits_{n \geq 1} \mathbb{E}(X_{n})^{+} < \infty\). Then there exists \(X \in L^{1}(\mathcal{B})\) so that \(X_{n} \to X\) almost surely.
\end{theorem}

Before preceding with the proof of theorem \ref{martingale_convergence}, we first establish a helpful lemma.

\begin{lemma}
    \label{submartingale_moments}
    Let $(X_{n})_{n \geq 1}$ be a discrete-time submartingale wrt filtration $\mathcal{F}_{*} := (\mathcal{F}_{n})_{n \in \mathbb{N}}$. Then $\sup\limits_{n \geq 1} \mathbb{E}(X_{n})^{+} < \infty$ iff $\sup\limits_{n \geq 1} \mathbb{E}|X_{n}| < \infty$.
\end{lemma}

\begin{proof}
    ($\Rightarrow$): Because $(X_{n})_{n \geq 1}$ is a submartingale, $\forall n \geq 1$,
    \begin{align*}
        &\mathbb{E}(X_{n}) \geq \mathbb{E}(X_{1})\\
        \Rightarrow&\mathbb{E}(X_{n})^{+} - \mathbb{E}(X_{n})^{-} \geq \mathbb{E}(X_{1})\\
        \Rightarrow&\mathbb{E}(X_{n})^{+} - \mathbb{E}(X_{1}) \geq \mathbb{E}(X_{n})^{-} \\
        \Rightarrow&\infty > \sup\limits_{n \geq 1} \mathbb{E}(X_{n})^{+} - \mathbb{E}(X_{1}) \geq \sup\limits_{n \geq 1} \mathbb{E}(X_{n})^{-}
    \end{align*}

    Thus,
    \begin{align*}
        \sup\limits_{n \geq 1} \mathbb{E}|X_{n}| &= \sup\limits_{n \geq 1} (\mathbb{E}(X_{n})^{+} + \mathbb{E}(X_{n})^{-})\\
        &\leq \sup\limits_{n \geq 1} (\mathbb{E}(X_{n})^{+}) + \sup\limits_{n \geq 1} (\mathbb{E}(X_{n})^{-})\\
        &< \infty 
    \end{align*}
    $\checkmark$

    ($\Leftarrow$): This follows because
    $$\infty > \sup\limits_{n \geq 1} \mathbb{E}|X_{n}| \geq \sup\limits_{n \geq 1} (\mathbb{E}(X_{n})^{+} + \mathbb{E}(X_{n})^{-}) \geq \sup\limits_{n \geq 1} \mathbb{E}(X_{n})^{+}$$
    $\checkmark$
\end{proof}

\begin{proof}[Proof of theorem \ref{martingale_convergence}]
    Letting $a < b \in \mathbb{R}$, we uniformly bound the number of complete upcrossings with proposition \ref{upcrossing_inequality}. First note that if $a \geq 0$ 
    $$(X_{n} - a)^{+} = 1_{X_{n} \geq a}X_{n} -1_{X_{n} \geq a}a \leq 1_{X_{n} \geq a}X_{n} \leq (X_{n})^{+}$$
    and if $a < 0$
    $$(X_{n} - a)^{+} = 1_{X_{n} \geq a}X_{n} -1_{X_{n} \geq a}a = (X_{n})^{+} + 1_{0 > X_{n} \geq a}X_{n} -1_{X_{n} \geq a}a \leq (X_{n})^{+} + |a|.$$

    So $\forall n \geq 1$
    \begin{align*}
        (b - a) \mathbb{E}(U_{n}[a, b]) &\leq \mathbb{E}(X_{n} - a)^{+} - \mathbb{E}(X_{1} - a)^{+}\\
        &\leq \mathbb{E}(X_{n})^{+} + |a|\\
        &\leq M + |a|
    \end{align*}
    for $\infty > M \geq \sup\limits_{n \geq 1} \mathbb{E}(X_{n})^{+}$. Therefore, we have that $\mathbb{E}(U_{n}[a,b]) \leq \frac{M + |a|}{b-a}$. Since $U_{n}[a,b]$ is non-decreasing as $n \to \infty$, it converges (possibly to $\infty$), say to $U[a,b]$. By monotone convergence theorem, we have that 
    $$\frac{M + |a|}{b-a} \geq \lim\limits_{n \to \infty} \mathbb{E}(U_{n}[a,b]) = \mathbb{E}(U[a,b]).$$
    Therefore, $U[a,b]$ must be finite almost surely (otherewise $\mathbb{E}(U[a,b]) = \infty$). This means that
    $$\mathbb{P}[\liminf\limits_{n \to \infty} X_{n} \leq a < b \leq \limsup\limits_{n \to \infty} X_{n}] = 0,$$
    since that event occurs iff $U[a,b] = \infty$.

    Therefore
    $$\bigcup\limits_{a, b \in \mathbb{Q}} [\liminf\limits_{n \to \infty} X_{n} \leq a < b \leq \limsup\limits_{n \to \infty} X_{n}]$$
    is a null set. But then we must have that $\liminf\limits_{n \to \infty} X_{n} = \limsup\limits_{n \to \infty} X_{n} =: X$ almost surely.

    Finally, we show that $X \in L^{1}(\mathcal{B})$. By lemma \ref{submartingale_moments}, $\infty > \sup\limits_{n \geq 1} \mathbb{E}|X_{n}|$. By Fatou's Lemma, we have that
    $$\mathbb{E}|X| \leq \liminf\limits_{n \to \infty} \mathbb{E}|X_{n}| \leq \sup\limits_{n \geq 1} \mathbb{E}|X_{n}| < \infty.$$
    so $X \in L^{1}(\mathcal{B})$.
\end{proof}

We end this section by looking at an example to help us understand what theorem \ref{martingale_convergence} tells us (and what it doesn't).

\textbf{Example 1} \cite{durrett_2010}: Consider the simple random walk $(X_n)_{n \geq 1}$ starting at \(0\) with increments of \(\pm 1\). Let \(\nu := \inf\limits \{n \geq 1 : X_{n} \geq 2\}\). Since \((X_n)_{n \geq 1}\) is a martingale and \(\nu\) is a stopping time, so \(X_{n \wedge \nu}\) is a martingale as well. Since \((X_n)_{n \geq 1}\) starts at \(0\) and moves by at most \(1\), we must have that \(\nu \geq 3\). So \(0 = \mathbb{E}(X_{1}) = \mathbb{E}(X_{1 \wedge \nu}) = \mathbb{E}(X_{n \wedge \nu})\) \(\forall n \geq 1\). Now certainly \(\sup\limits_{n \geq 1} \mathbb{E}(X_{n \wedge \nu})^{+} \leq 2\), so \(X_{n \wedge \nu} \to X\) almost surely by theorem \ref{martingale_convergence}. Since \(X_{n \wedge \nu}\) only takes integral values, \(X\) must (almost surely) as well. But since \(X_{n \wedge \nu}\) changes by \(\pm 1\) if it is not already at \(2\), \(X\) must be \(2\). Thus, \(\mathbb{E}(X) = 2 \neq 0 = \mathbb{E}(X_{n \wedge \nu})\) for \(n \geq 1\). Therefore \((X_{n \wedge \nu})_{n \geq 1}\) cannot converge in \(L^{1}(\mathcal{B})\).

This example seems to fly in our face of what it means to be a "fair game". How can \((X_{n \wedge \nu})_{n \geq 1}\) be a martingale, but converge to something with larger expectation almost surely? If we were to play finitely many games, then the probability mass assigned to positive results \(1\) and \(2\) will be matched equally by the probability mass asssigned to negative results. That is, although it is almost sure that at some \(n\) we will hit \(X_{n} = 2\), we may suffer very large losses before that happens. 

% \textbf{Example 2}:

