\section{Martingales}
\label{stoch:martingales}

We start our initial study of martingales by first setting up some underlying concepts. Again, we let \((\Omega, \mathcal{B}, \mathbb{P})\) be our default probability space.

\begin{definition}
    Let \(T\) be a total ordering and let \(S, \Sigma\) be a measure space. A \textbf{stochastic process} is a random element \(X: \Omega, \mathcal{B} \to S^{T}, \Sigma^{T}\), where \(\Sigma^{T}\) denotes the \(\Sigma\)-product sigma algebra on \(S^{T}\). We call \(S, \Sigma\) the \textbf{state space}.
\end{definition}

Normally, however, we think of a stochastic process as some kind random object that progresses through "time". We could alternately have defined it as an indexed collection of random elements \(\{X_{t}: \Omega, \mathcal{B} \to S, \Sigma\}_{t \in T}\). The definitions are equivalent due to the following proposition

\begin{proposition}[\cite{folland_2011}]
    Let \((Z, \mathcal{M})\), $(Y_{\alpha}, \mathcal{N}_\alpha)$ be measure spaces for $\alpha \in A$. Let $Y = \prod\limits_{\alpha \in A} Y_\alpha$ and let $\mathcal{N} = \bigotimes\limits_{\alpha \in A} \mathcal{N}_\alpha$. Let $f : Z \to Y$ and denote $f_{\alpha} := \pi_{\alpha} \circ f$, where $\pi_\alpha$ is the projection map for $\alpha \in A$. Then $f$ is $(\mathcal{M}, \mathcal{N})$-measureable iff $f_\alpha$ is $(\mathcal{M}, \mathcal{N}_\alpha)$-measureable for every $\alpha \in A$.
\end{proposition}

Therefore, we will use the two definitions interchangeably.

The total ordering in the definition of stochastic process gives us our concept of "time". Usually, \(T = \mathbb{N}\) or \(\mathbb{R}\). However, we can imagine more exotic stochastic processes by choosing a total ordering that cannot be order embedded into \(\mathbb{R}\). For example, we could consider \(\mathbb{R}^{2}, \preccurlyeq\), endowed with \(\preccurlyeq\) is the lexicographic ordering of \(\mathbb{R}^{2}\). Observe that 
\[\{\{t\} \times \mathbb{R} \subset \mathbb{R}^{2} | t \in \mathbb{R}\}\]
is a partition of \(\mathbb{R}^{2}\) into uncountably many nondegenerate \(\preccurlyeq\)-intervals. Thus, if \(\mathbb{R}^{2}, \preccurlyeq\) could be order embedded into \(\mathbb{R}\), then \(\mathbb{R}\) would uncountably many disjoint nondegenerate intervals, each of which must contain a rational. But then the rationals would be uncountable, giving us a contradiction \cite{exotic_order}.

A stochastic process defined with this more exotic total ordering would look like a grid of random variables, with each vertical slice representing a stochastic process over \(\mathbb{R}\). We could instead choose to think of it as a stochastic process of stochastic processes indexed by \(\mathbb{R}\).

A stochastic process on its own does not have much more structure than a random element. We would like to incorporate the idea of measureability with respect to some growing information.

\begin{definition}
    A \textbf{filtration} is an non-decreasing collection of sub-sigma algebras \(\{\mathcal{F}_{t} \subset \mathcal{B}\}_{t \in T}\), indexed by our total ordering \(T\).
\end{definition}

\begin{definition}
    An \textbf{adapted process} with respect to filtration \(\{\mathcal{F}_{t}\}_{t \in T}\) is a stochastic process \(\{X_{t}\}_{t \in T}\) so that \(\forall t \in T\), \(X_{t} \in \mathcal{F}_{t}\).
\end{definition}

Finally, we are ready to give our definition of a martingale, our concept of a fair game.

\begin{definition}
    \label{martingale_defn}
    Let $X: \Omega \to \mathbb{R}^{T}$ be an adapted process wrt filtration $\mathcal{F}_{*} = (\mathcal{F}_{t})_{t \in T}$. $X$ is a \textbf{martingale} if it satisfies the following properties:
    \begin{enumerate}
        \item $X_{t} \in L^{1}(\mathcal{B})$ $\forall t \in T$,
        \item $\mathbb{E}(X_{t} | \mathcal{F}_{r}) = X_{r}$ (\(\mathbb{P}\)-a.s.) for any $r \leq t$.
    \end{enumerate}
\end{definition}

Note that for condition (2) of definition \ref{martingale_defn}, we need only check that it holds for \(r < t\), since it already holds for \(r = t\) by the assumption that \(X\) is an adapted process and by the known information property of proposition \ref{cexpe_properties}.

We can also define the related sub(super)-martingale by changing the equality in condition (2) of definition \ref{martingale_defn} to a \(\geq\) (\(\leq\)). A submartingale represents a favorable game and a supermartingale represents an unfavorable game. We could have alternatively defined submartingale and supermartingale first, then defined a martingale as an adapted process that is both a submartingale and a supermartingale. We will later prove theorems about submartingales, but they will automatically carry over to martingales. To get the corresponding property for supermartingales, we can just look at the supermartingale's negation to turn it into a submartingale.

Note that by applying \textit{smoothing} and the identity \(\mathbb{E}(Y) = \mathbb{E}(Y|\{\emptyset, \Omega\})\), we get that for any \(r \leq t \in T\), 
\[\mathbb{E}(X_{t}) = \mathbb{E}(X_{t} | \{\emptyset, \Omega\}) = \mathbb{E}(\mathbb{E}(X_{t} | \mathcal{F}_{r})|\{\emptyset, \Omega\}) = \mathbb{E}(X_{r}|\{\emptyset, \Omega\}) = \mathbb{E}(X_{r})\]

When our total ordering \(T = \mathbb{N}\), we have the following alternative characterization of a martingale
\begin{proposition}
    \label{natural_martingale}
    Let $X: \Omega \to \mathbb{R}^{\mathbb{N}}$ be an adapted process wrt filtration $\mathcal{F}_{*} = (\mathcal{F}_{n})_{n \in \mathbb{N}}$. $X$ is a martingale if and only if it satisfies the following properties:
    \begin{enumerate}
        \item $X_{n} \in L^{1}(\mathcal{B})$ $\forall n \in \mathbb{N}$,
        \item $\mathbb{E}(X_{n+1} | \mathcal{F}_{n}) = X_{n}$ (\(\mathbb{P}\)-a.s.) for any $n \in \mathbb{N}$.
    \end{enumerate}
\end{proposition}

\begin{proof}
    \((\Rightarrow)\) Condition (2) of \ref{natural_martingale} is just a special case of condition (2) of \ref{martingale_defn} when we set \(t = n+1 \geq n = r\).

    \((\Leftarrow)\): This follows by induction. First observe that for $n = 1$, the only $m \in \mathbb{N}$ s.t. $m \leq n$ is $m = 1$. Therefore, $\forall m \leq n$, we have that $\mathbb{E}(X_{n} | \mathcal{F}_{m}) = \mathbb{E}(X_{1} | \mathcal{F}_{1}) = X_{1}$. For the inductive step, suppose this claim holds for $n \in \mathbb{N}$. Consider $m \leq n+1$. If \(m = n+1\), then \(\mathbb{E}(X_{n+1} | \mathcal{F}_{m}) = \mathbb{E}(X_{n+1} | \mathcal{F}_{n+1}) = X_{n+1}\), so we're good. On the other hand, if \(m < n+1\), then we have that $$\mathbb{E}(X_{n+1} | \mathcal{F}_{m}) = \mathbb{E}(\mathbb{E}(X_{n+1} | \mathcal{F}_{n}) | \mathcal{F}_{m}) = \mathbb{E}(X_{n} | \mathcal{F}_{m}) = X_{m}$$ by the \textit{smoothing} property of proposition \ref{cexpe_properties} and induction.
\end{proof}

We refer to such martingales as \textbf{discrete-time} martingales.

TODO:
- \st{definition of stochastic process}
- \st{remark about total ordering}
- \st{definition of adapted process and filter}
- \st{definition of martingale (total ordering), super, sub}
- \st{connection to natural number definition}

\section{Decomposition of Discrete-Time martingales}

A useful concept for discrete-time martingales is a predictable process

\begin{definition}
    Let $X: \Omega \to S^{\mathbb{N}}$ be an adapted process wrt filtration $(\mathcal{F}_{n})_{n \in \mathbb{N}}$ and state space \(S, \Sigma\). \(X\) is a \textbf{predictable process} if for all \(n \in \mathbb{N}\), \(X_{n} \in \mathcal{F}_{n-1}\), with \(\mathcal{F}_{0} = \{\emptyset, \Omega\}\).
\end{definition}

This concept is capturing the idea that we can "predict" the next value of a stochastic process \(X\) using the information we have now. Indeed, for a real-valued predictable process, if at timestep \(n\) we can discern between whether a given event in \(\mathcal{F}_{n}\) occured, then for any \(x \in \mathbb{R}\) we can determine whether or not \(X_{n+1} = x\) since \([X_{n+1}=x] \in \mathcal{F}_{n}\). By setting \(\mathcal{F}_{0} = \{\emptyset, \Omega\}\), we enforce that \(X_{1}\) is a constant.

The following proposition reveals that we can think of any adapted process indexed by \(\mathbb{N}\) as a predictable process with some "fair" noise.

\begin{proposition}
    \label{process_decomposition}
    Let $X: \Omega \to \mathbb{R}^{\mathbb{N}}$ be a discrete-time adapted process wrt filtration $\mathcal{F}_{*} := (\mathcal{F}_{n})_{n \in \mathbb{N}}$ so that \(X_{n} \in L^{1}(\mathcal{B})\) for all \(n \geq 1\). Then there exists a martingale \((M_{n})_{n \geq 1}\) and a predictable process \((A_{n})_{n \geq 1}\) with \(A_{1} = 0\) so that for \(n \geq 1\)
    \[X_{n} = M_{n} + A_{n}.\]
    This decomposition is \(\mathbb{P}\)-a.s. unique.
\end{proposition}

Before we proceed with the proof of proposition \ref{process_decomposition}, we first establish a useful concept and lemma.

\begin{definition}
    \label{difference_defn}
    Let $d: \Omega \to \mathbb{R}^{\mathbb{N}}$ be a discrete-time process adapted to filtration $(\mathcal{F}_{n})_{n \in \mathbb{N}}$. Then, it is a \textbf{martingale difference sequence} if
    \begin{enumerate}
        \item $d_{j} \in L^{1}(\mathcal{B})$ $\forall j \in \mathbb{N}$
        \item $\mathbb{E}(d_{j+1} | \mathcal{B}_{j}) = 0$ \(\forall j \in \mathbb{N}\)
    \end{enumerate}
\end{definition}

\begin{lemma}
    \label{difference_martingale_equiv}
    \((M_{n})_{n \geq 1}\) is a martingale adapted to filtration \(\mathcal{F}_{*} = (\mathcal{F}_{n})_{n \geq 1}\) iff there exists a martingale difference sequence $d: \Omega \to \mathbb{R}^{\mathbb{N}}$  adapted to the same filtration and \(c \in \mathbb{R}\) so that
    \[M_{n} := c + \sum\limits_{j=1}^{n} d_{j}\]
    for all \(n \in \mathbb{N}\).
\end{lemma}

\begin{proof}
    \((\Rightarrow)\): Let \(d_{n} := M_{n} - M_{n-1}\) for \(n \geq 1\), where \(M_{0} = \mathbb{E}(M_{1})\). Then for \(n \geq 1\), the sum \(M_{0} + \sum\limits_{j=1}^{n} d_{j} = M_{0} + \sum\limits_{j=1}^{n} M_{j} - M_{j-1}\) is telescoping and is equal to \(M_{n}\).
    
    Now we need simply show that \((d_{n})_{n \geq 1}\) is a martingale difference sequence. First note that \((d_{n})_{n \geq 1}\) is adapted to \(\mathcal{F}_{*}\) since \(M_{n} - M_{n-1} \in \mathcal{F}_{n}\) for all \(n \geq 1\). Checking condition (1) of \ref{difference_defn}, we see for \(n \geq 1\) that \(\mathbb{E}|d_{n}| = \mathbb{E}|M_{n} - M_{n-1}| \leq \mathbb{E}|M_{n}| + \mathbb{E}|M_{n-1}|< \infty\), so \(d_{n} \in L^{1}(\mathcal{B})\).

    Checking condition (2), we see that
    \[\mathbb{E}(d_{n+1}|\mathcal{F}_{n}) = \mathbb{E}(M_{n+1}|\mathcal{F}_{n})- \mathbb{E}(M_{n}|\mathcal{F}_{n})) = M_{n} - M_{n} = 0. \]

    Both conditions are satisfied so \((d_{n})_{n \geq 1}\) is a martingale difference sequence. \(\checkmark\)

    \((\Leftarrow)\): We are given difference sequence \((d_{n})_{n \geq 1}\) and constant \(c \in  \mathbb{R}\). Note that since \((d_{n})_{n \geq 1}\) is adapted to \(\mathcal{F}_{*}\), we know \(M_{n} = c + \sum\limits_{j=1}^{n} d_{j} \in \mathcal{F}_{n}\) for all \(n \geq 0\), so \((M_{n})_{n \geq 1}\) is adapted to \(\mathcal{F}_{*}\). Now, checking for \(n \in \mathbb{N}\) that \(M_{n} \in L^{1}(\mathcal{B})\), consider
    \begin{align*}
        \mathbb{E}|M_{n}| &= \mathbb{E}\left| c + \sum\limits_{j=1}^{n} d_{j}  \right| \\
        & \leq  |c| + \sum\limits_{j=1}^{n} \mathbb{E}| d_{j} | \\
        & < \infty
    \end{align*}
    since \(d_{j} \in L^{1}(\mathcal{B})\) \(\forall j \in \mathbb{N}\).

    Next we check condition (2) of definition \ref{martingale_defn}. Letting \(n \in \mathbb{N}\)
    \begin{align*}
        \mathbb{E}(M_{n+1}|\mathcal{F}_{n}) &= c + \sum\limits_{j=1}^{n+1} \mathbb{E}(d_{j}|\mathcal{F}_{n}) \\
        &= \mathbb{E}(d_{n+1}|\mathcal{F}_{n}) +  c + \sum\limits_{j=1}^{n} d_{j} \\
        &=c + \sum\limits_{j=1}^{n} d_{j} & ((d_{n})_{n \geq 1} \text{is a difference sequence})\\
        &= M_{n}.
    \end{align*}

    Both conditions are satisfied, so \((M_{n})_{n \geq 1}\) is a martingale. \(\checkmark\)
\end{proof}

Note that our proof above also gives a procedure to construct a martingale difference sequence from a martingale. Now we proceed to the proof of proposition \ref{process_decomposition}
\begin{proof}[Proof of Proposition \ref{process_decomposition}]
    We will deconstruct \((X_{n})_{n \geq 1}\) into
    \begin{align*}
        M_{n} &:= X_{0} + \sum\limits_{j=1}^{n} X_{j} - \mathbb{E}(X_{j}|\mathcal{F}_{j-1}) \\
        A_{n} &:=  \sum\limits_{j=1}^{n} \mathbb{E}(X_{j}|\mathcal{F}_{j-1}) - X_{j-1} \\
    \end{align*}
    where we let \(\mathcal{F}_{0} := \{\emptyset, \Omega\}\) and \(X_{0} = \mathbb{E}(X_{1}|\mathcal{F}_{0}) = \mathbb{E}(X_{1})\). Observe that \(A_{1} = \mathbb{E}(X_{1}|\mathcal{F}_{0}) - X_{0} = \mathbb{E}(X_{1}) - \mathbb{E}(X_{1}) = 0\). With this choice of \((M_{n})_{n \geq 1}\) and \((A_{n})_{n \geq 1}\) we have that
    \begin{align*}
        M_{n} + A_{n} &= X_{0} + \sum\limits_{j=1}^{n} X_{j} - \mathbb{E}(X_{j}|\mathcal{F}_{j-1}) + \sum\limits_{j=1}^{n} \mathbb{E}(X_{j}|\mathcal{F}_{j-1}) - X_{j-1} \\
        &=  X_{0} + \sum\limits_{j=1}^{n} X_{j} - \mathbb{E}(X_{j}|\mathcal{F}_{j-1}) + \mathbb{E}(X_{j}|\mathcal{F}_{j-1}) - X_{j-1} \\
        &= X_{0} + \sum\limits_{j=1}^{n} X_{j} - X_{j-1} \\
        &= X_{n}.
    \end{align*}
    Next we show that \((M_{n})_{n \geq 1}\) is a martingale and \((A_{n})_{n \geq 1}\) is a predictable sequence. To that end, observe that for \(n \geq 1\):
    \begin{enumerate}
        \item \(X_{n} - \mathbb{E}(X_{n}|\mathcal{F}_{n-1}) \in \mathcal{F}_{n}\) since \((X_{n})_{n \geq 1}\) is an adapted process and \(\mathcal{F}_{n-1} \subset \mathcal{F}_{n}\).
        \item We are given that \(X_{n} \in L^{1}(\mathcal{B})\) and \(\mathbb{E}(X_{n}|\mathcal{F}_{n-1})\in L^{1}(\mathcal{B})\) by definition, so \(X_{n} - \mathbb{E}(X_{n}|\mathcal{F}_{n-1}) \in L^{1}(\mathcal{B})\).
        \item \(\mathbb{E}(X_{n} - \mathbb{E}(X_{n}|\mathcal{F}_{n-1})|\mathcal{F}_{n-1})= \mathbb{E}(X_{n}|\mathcal{F}_{n-1}) - \mathbb{E}(X_{n}|\mathcal{F}_{n-1}) = 0\)
    \end{enumerate}
    so \(X_{n} - \mathbb{E}(X_{n}|\mathcal{F}_{n-1})\) is a martingale difference sequence. By lemma \ref{difference_martingale_equiv}, noting that \(X_{0} = \mathbb{E}(X_{1})\) is a constant, we have that \( M_{n} = X_{0} + \sum\limits_{j=1}^{n} X_{j} - \mathbb{E}(X_{j}|\mathcal{F}_{j-1})\) is a martingale.

    On the other hand, for all \(j \geq 1\), we know that \(\mathbb{E}(X_{j}|\mathcal{F}_{j-1}) - X_{j-1} \in \mathcal{F}_{j}\) since \((X_{n})_{n \geq 1}\) is an adapted process. Because \(\mathcal{F}_{*}\) is a filtration, for all \(m \geq j\), \(\mathcal{F}_{j} \subset \mathcal{F}_{m}\). Therefore, for all \(n \geq 1\), \(A_{n} =  \sum\limits_{j=1}^{n} \mathbb{E}(X_{j}|\mathcal{F}_{j-1}) - X_{j-1} \in \mathcal{F}_{n}\) and it is predictable.

    Finally we show that this decomposition is (almost surely) unique. Suppose there existed another decomposition with the same properties: \(X_{n} = N_{n} + B_{n}\). We will apply induction to show that these decompositions are almost surely the same. For our base case, let \(n = 1\). We are given that \(A_{1} = B_{1} = 0\). Therefore, after subtracting the two decompositions, \(0 = M_{1} + A_{1} - N_{1} + B_{1} = M_{1} - N_{1}\), so \(M_{1} = N_{1}\). For our inductive step, assume for some \(n \geq 1\) that \(M_{n} = N_{n}\) and \(A_{n} = B_{n}\) almost surely. We know that \(0 = M_{n+1} + A_{n+1} - N_{n+1} - B_{n+1}\). Taking conditional expectations of both sides conditioned on \(\mathcal{F}_{n}\), we get
    \begin{align*}
        0 &= \mathbb{E}(M_{n+1} + A_{n+1} - N_{n+1} - B_{n+1}|\mathcal{F}_{n}) \\
        &= M_{n} - N_{n} + \mathbb{E}(A_{n+1}|\mathcal{F}_{n}) - \mathbb{E}(B_{n+1}|\mathcal{F}_{n}) & ((M_{n})_{n \geq 1}, (N_{n})_{n \geq 1} \text{ are martingales})\\
        &= M_{n} - N_{n} + A_{n+1} - B_{n+1} & ((A_{n})_{n \geq 1}, (B_{n})_{n \geq 1} \text{ are predictable}) \\
        &= A_{n+1} - B_{n+1} & \text{(inductive hypothesis)} \\
    \end{align*}

    Therefore \(A_{n+1} = B_{n+1}\) almost surely. Then \(0 = M_{n+1} + A_{n+1} - N_{n+1} - B_{n+1} = M_{n+1} - N_{n+1}\), so \(M_{n+1} = N_{n+1}\) almost surely, proving the inductive hypothesis for \(n+1\). Therefore, by induction, we have that  \(M_{n+1} + A_{n+1}\) is the almost surely unique decomposition of \(X_{n}\) for \(n \geq 1\) with the desired properties.
\end{proof}

We can interpret proposition \ref{process_decomposition} as saying any adapted process is some kind of "trend" with "fair" additive noise applied on top. This result naturally gives us some well-known decomposition results.

\begin{corollary}[Doob's Submartingale Decomposition]
    \label{doob_decomposition}
    Let \((Y_{n})_{n \geq 1}\) be a discrete-time adapted process wrt filtration \(\mathcal{F}_{*} := (\mathcal{F}_{n})_{n \geq 1}\) so that \(Y_{n} \in L^{1}(\mathcal{B})\) for all \(n \geq 1\). Then it is a submartingale iff there exists a martingale \((M_{n})_{n \geq 1}\) and a non-decreasing predictable process \((A_{n})_{n \geq 1}\) with \(A_{1} = 0\) so that for \(n \geq 1\)
    \[Y_{n} = M_{n} + A_{n}\]
    This decomposition is almost surely unique.
\end{corollary}

\begin{proof}
    \((\Rightarrow)\): Apply proposition \ref{process_decomposition} to get our almost surely unique martingale \((M_{n})_{n \geq 1}\) and predictable process \((A_{n})_{n \geq 1}\) so that \(Y_{n} = M_{n} + A_{n}\) for \(n \geq 1\). We must simply show that \(A_{n}\) is non-decreasing. To that end, observe that for \(n \geq 2\)
    \begin{align*}
        0 &\leq \mathbb{E}(Y_{n}|\mathcal{F}_{n-1}) - Y_{n-1} \\
        &= \mathbb{E}(M_{n} + A_{n}|\mathcal{F}_{n-1}) - A_{n-1} \\
        &= \mathbb{E}(M_{n}|\mathcal{F}_{n-1})  - M_{n-1} + \mathbb{E}(A_{n}|\mathcal{F}_{n-1}) - A_{n-1} \\
        &= M_{n-1} - M_{n-1} + \mathbb{E}(A_{n}|\mathcal{F}_{n-1}) - A_{n-1} &((M_{n})_{n \geq 1} \text{is a martingale}) \\
        &=A_{n}- A_{n-1}, &((A_{n})_{n \geq 1} \text{is a predictable process})
    \end{align*}
    giving us that \((A_{n})_{n \geq 1}\) is non-decreasing. \(\checkmark\)

    \((\Leftarrow)\): We must show that \(Y_{n} = M_{n} + A_{n}\) for \(n \geq 1\) is a submartingale. We only need to check condition (2) since we are given \(Y_{n} \in L^{1}(\mathcal{B})\). Checking, for \(n \geq 1\), we see
    \begin{align*}
        \mathbb{E}(Y_{n+1}|\mathcal{F}_{n}) &= \mathbb{E}(M_{n+1} + A_{n+1}|\mathcal{F}_{n}) \\
        &= \mathbb{E}(M_{n+1}|\mathcal{F}_{n}) + A_{n+1} \\
        &= M_{n} + A_{n+1} & ((M_{n})_{n \geq 1} \text{ is a martingale})\\
        &\geq M_{n} + A_{n} & ((A_{n})_{n \geq 1} \text{ is non-decreasing})\\
        &= Y_{n},
    \end{align*}
    so \((Y_{n})_{n \geq 1}\) is indeed a submartingale. \(\checkmark\)
\end{proof}


TODO: 
- \st{predictable process definition}
- \st{decomp of adapted process into predictable process and martingale}
- \st{Doob's Decomposition}

\section{Strategies and Discrete-Time martingales}

To further analyze martingales, we introduce the concept of a "strategy" for playing the "fair game", where we our strategy determines our move at the end of each turn.

\begin{definition}
    Let $X: \Omega \to \mathbb{R}^{\mathbb{N}}$ be a discrete-time adapted process wrt filtration $\mathcal{F}_{*} := (\mathcal{F}_{n})_{n \in \mathbb{N}}$ and let $H: \Omega \to \mathbb{R}^{\mathbb{N}}$ be a predictable process on the same filtration. Then we call \(H\) a \textbf{strategy} for \(X\) with \textbf{winnings} 
    \[(H \cdot X)_{n} := \sum\limits_{m=1}^{n} H_{m} (X_{m} - X_{m-1})\]
    for $n \geq 1$ (where \(X_{0} = 0\)).
\end{definition}

The following proposition will establish that we cannot change the fairness of a game by choosing the "best" (or "worst") strategy.

\begin{proposition}
    \label{fairness_preservation}
    Let $(Y_{n})_{n \geq 1}$ be a discrete-time martingale wrt filtration $\mathcal{F}_{*} := (\mathcal{F}_{n})_{n \in \mathbb{N}}$ and let \((H_{n})_{n \geq 1}\) be bounded strategy for \((Y_{n})_{n \geq 1}\). Then \(((H \cdot Y)_{n})_{n \geq 1}\) is a martingale.
\end{proposition}
\begin{proof}
    First note that since \(Y_{0}, Y_{1}, \dots, Y_{n}, H_{1}, \dots, H_{n} \in \mathcal{F}_{n}\), we have that \((H \cdot Y)_{n} \in \mathcal{F}_{n}\), so \((H \cdot Y)_{n}\) is adapted to \(\mathcal{F}_{*}\).
    
    Next note that for $n \geq 1$ because $H_{n}$ is bounded, say by $M \in \mathbb{R}_{\geq 0}$ 
    \begin{align*}
        \mathbb{E}|(H \cdot Y)_{n}| &= \mathbb{E}\left|\sum\limits_{m=2}^{n}H_{m}(Y_{m} - Y_{m-1})\right|\\
        &\leq M \sum\limits_{m=1}^{n} \mathbb{E}|Y_{m} - Y_{m-1}| & \text{(Triangle Inequality)}\\
        &\leq M \sum\limits_{m=1}^{n} \mathbb{E}|Y_{m}| + \mathbb{E}| Y_{m-1}| & \text{(Triangle Inequality)}\\
        &< \infty
    \end{align*}
    since $Y_{m} \in L^{1}(\mathcal{B})$ $\forall m \geq 0$. So $(H \cdot Y)_{n} \in L^{1}(\mathcal{B})$ as well. $\checkmark$

    Observe that for $n \geq 1$
    \begin{align*}
        \mathbb{E}((H \cdot Y)_{n+1} | \mathcal{F}_{n}) &= \mathbb{E}(\sum\limits_{m=1}^{n+1} H_{m}(Y_{m} - Y_{m-1}) | \mathcal{F}_{n})\\
        &=\sum\limits_{m=1}^{n+1}\mathbb{E}(H_{m}(Y_{m} - Y_{m-1}) | \mathcal{F}_{n})\\
        &=\sum\limits_{m=1}^{n}H_{m}(Y_{m} - Y_{m-1}) + \mathbb{E}(H_{n+1} (Y_{n+1} - Y_{n}) | \mathcal{F}_{n})\\
        &= (H \cdot Y)_{n}
    \end{align*}
    since \((Y_{n})_{n \geq 1}\) is a martingale, so
    \begin{align*}
        \mathbb{E}(H_{n+1} (Y_{n+1} - Y_{n}) | \mathcal{F}_{n}) &= H_{n+1} (\mathbb{E}(Y_{n+1} | \mathcal{F}_{n}) -Y_{n})\\
        &= 0. \checkmark
    \end{align*}
     
    Therefore, we have that \(((H \cdot Y)_{n})_{n \geq 1}\) is a martingale.
\end{proof}

Using the Doob decomposition, we can easily generalize this result to submartingales (and therefore supermartingales by negation):

\begin{corollary}
    \label{favorability_preservation}
    Let $(Y_{n})_{n \geq 1}$ be a discrete-time submartingale wrt filtration $\mathcal{F}_{*} := (\mathcal{F}_{n})_{n \in \mathbb{N}}$ and let \((H_{n})_{n \geq 1}\) be non-negative bounded strategy for \((Y_{n})_{n \geq 1}\). Then \(((H \cdot Y)_{n})_{n \geq 1}\) is a submartingale.
\end{corollary}
    
\begin{proof}
    Using corollary \ref{doob_decomposition}, decompose \((Y_{n})_{n \geq 1}\) into \((M_{n} + A_{n})_{n \geq 1}\), where \((M_{n})_{n \geq 1}\) is a martingale and \((A_{n})_{n \geq 1}\) is a predictable process with \(A_{1} = 0\). Denote \(M_{0} = A_{0} = 0\) to make \(Y_{0} = M_{0} + A_{0} = 0\). Then,
    \begin{align*}
        (H \cdot Y)_{n} &= \sum\limits_{m=1}^{n}H_{m}(Y_{m} - Y_{m-1}) \\
        &= \sum\limits_{m=1}^{n}H_{m}(M_{m} + A_{m} - M_{m-1} - A_{m-1}) \\
        &= \sum\limits_{m=1}^{n}H_{m}(M_{m} - M_{m-1}) + \sum\limits_{m=1}^{n}H_{m}( A_{m}  - A_{m-1}) \\
        &= (H \cdot M)_{n} + (H \cdot A)_{n}
    \end{align*}
    We know by proposition \ref{fairness_preservation} that \(((H \cdot M)_{n})_{n \geq 1}\) is a martingale. Furthermore, we have that 
    \begin{enumerate}
        \item \(((H \cdot A)_{n})_{n \geq 1}\) is a predictable process because  \((H_{n})_{n \geq 1}\) and \((A_{n})_{n \geq 1}\) are predictable processes.
        \item \((H \cdot A)_{1} = H_{1} A_{1} = 0\).
        \item For all \(n \geq 1\), \((H \cdot A)_{n+1} = (H \cdot A)_{n} + H_{n+1} (A_{n+1} - A_{n}) \geq (H \cdot A)_{n}\) because \(H_{n+1} \geq 0\) and \(A_{n+1} \geq A_{n}\) (it is non-decreasing). Thus \(((H \cdot A)_{n})_{n \geq 1}\) is a non-decreasing sequence. 
    \end{enumerate}

    Therefore, by the converse case of corollary \ref{doob_decomposition}, we have that  \(((H \cdot Y)_{n})_{n \geq 1} = ( (H \cdot M)_{n} + (H \cdot A)_{n})_{n \geq 1}\) is a submartingale.
\end{proof}

Corresponding results hold for submartingales and martingales. If $(Y_{n})_{n \geq 1}$ is a martingale, then bounded \((H_{n})_{n \geq 1}\) will suffice, it does not need to be non-negative. 

We can look at playing the game up to a stopping point as a special kind of strategy. The classical definition of a "stopping time" is as follows

\begin{definition}
    Let $\mathcal{F}_{*} := (\mathcal{F}_{n})_{n \in \mathbb{N}}$ be a filtration. A stopping time on \(\mathcal{F}_{*}\) is a random variable \(\nu: \Omega \to \bar{\mathbb{N}}\) so that for each \(n \in \mathbb{N}\), \([\nu = n] \in \mathcal{F}_{n}\). Here, \(\bar{\mathbb{N}}\) is the extended natural numbers.
\end{definition}

There are a couple equivalent conditions for being a stopping time. 
\begin{proposition}
    \label{stopping_time_equiv_conds}
    Suppose $\mathcal{F}_{*} := (\mathcal{F}_{n})_{n \in \mathbb{N}}$ is a filtration on $\Omega$. Let $\nu : \Omega \to \bar{\mathbb{N}}$. The following conditions are equivalent:
    \begin{enumerate}
        \item $\nu$ is a stopping time.
        \item $[\nu \leq n] \in \mathcal{F}_{n}, \forall n \in \mathbb{N}$
        \item $[\nu > n] \in \mathcal{F}_{n}, \forall n \in \mathbb{N}$
    \end{enumerate}
\end{proposition}

\begin{proof}
    $(1) \Rightarrow (2)$: We have for $n \in \mathbb{N}$ 
    $$[\nu \leq n] = \bigcup\limits_{m \leq n}[\nu =m] \in \mathcal{F}_{n}$$
    since $[\nu = m] \in \mathcal{F}_{m} \subset \mathcal{F}_{n}$ $\forall m \leq n$. $\checkmark$

    $(2) \Rightarrow (3)$: Let $n \in \mathbb{N}$. Since $[\nu \leq n] \in \mathcal{F}_{n}$, we know by definition of a sigma-algebra that $[\nu > n] = [\nu \leq n]^{C} \in \mathcal{F}_{n}$. $\checkmark$

    $(3) \Rightarrow (1)$: Let $n \in \mathbb{N}$. Since $[\nu > n] \in \mathcal{F}_{n}$ and because $\mathcal{F}_{n}$ forms a filtration, we have that
    $$[\nu = n] =  [\nu > n-1] \cap ([\nu > n])^{C} \in \mathcal{F}_{n},$$
    where $[\nu > 0] = \Omega$. $\checkmark$
\end{proof}

We can alternatively view a stopping time as a special kind of strategy:

\begin{proposition}
    Suppose $\mathcal{F}_{*} := (\mathcal{F}_{n})_{n \in \mathbb{N}}$ is a filtration on $\Omega$. Let $\nu : \Omega \to \bar{\mathbb{N}}$. If \(\nu\) is a stopping time, then \((1_{\nu \geq n})_{n \geq 1}\) is a predictable process.
\end{proposition}

\begin{proof}
    We need only check that \(1_{\nu \geq n} \in \mathcal{F}_{n-1}\) for \(n \geq 1\) (with \(\mathcal{F}_{0} = \{\emptyset, \Omega\}\)). To that end we have \(1_{\nu \geq 0} = 1\) since \(\nu(\omega) \in \bar{\mathbb{N}}\) for all \(\omega \in \Omega\), so \(1_{\nu \geq 0} \in \mathcal{F}_{0}\). For \(n \geq 1\), we have that \(1_{\nu \geq n} = 1 - 1_{\nu \leq n-1} \in \mathcal{F}_{n-1}\) since \([\nu \leq n-1] \in \mathcal{F}_{n-1}\) by proposition \ref{stopping_time_equiv_conds}.
\end{proof}

For a adapted process $X: \Omega \to \mathbb{R}^{\mathbb{N}}$, we can denote the process resulting from stopping the game after \(\nu\) turns as:
\[X_{n \wedge \nu} := (1_{\nu \geq \cdot} \cdot X)_{n} = \sum\limits_{m=1}^{n} 1_{\nu \geq n} (X_{m} - X_{m-1}).\]

Combining with our result about strategies, we get the following result:

\begin{corollary}
    Let $(X_{n})_{n \geq 1}$ be a discrete-time (sub/super)martingale wrt filtration $\mathcal{F}_{*} := (\mathcal{F}_{n})_{n \in \mathbb{N}}$ and let \(\nu\) be stopping time on \(\mathcal{F}_{*}\). Then \((X_{n \wedge \nu})_{n \geq 1}\) is a (sub/super)martingale.
\end{corollary}

\begin{proof}
    This follows from the definition of \(X_{n \wedge \nu}\) and application of proposition \ref{fairness_preservation} or corollary \ref{favorability_preservation}.
\end{proof}

TODO:
- \st{Definition}
- \st{preservation of fairness}
- \st{relationship to stopping times}

\section{Convergence of Discrete-time Martingales}

TODO:
- Upcrossing inequality
- convergence of discrete-time martingales
- pathological examples

\section{Cotninuous-time martingales, definitions}
- indistinguishable
- section 1.1 Karatzas shreve

\section{Continuous-time martingales, stopping times}
-section 1.2 Karatzas shreve

\section{Convergence of Continuous-time Martingales}
- section 1.3 karatzas shreve
- submartingale inequalities (Karatzas thm 3.8)
- convergence of right-continuous martingales