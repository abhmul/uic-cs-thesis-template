\section{Differential Equation Method}
\label{example:diffeq}

In this section we examine an example studied in \cite{diffeq_2020} about counting the number of connected components of size \(k\) in an Erdős-Rényi graph. First we  construct our probability space. 

We let \(\mathbb{G}_{n}\) be the set of graphs on \([n]\) for some \(n \geq 0\). Denote \(\binom{[n]}{2}\) the set of edges of the complete graph on \([n]\). Each graph in \(G \in \mathbb{G}_{n}\) has the same vertex set, so they can be identified with their edge sets \(E(G) \subset \binom{[n]}{2}\). Thus, we simply refer to a graph in \(\mathbb{G}_{n}\) by its set of edges. 

Let \(n \in \mathbb{N}\), let \(\Omega :=  \binom{[n]}{2}\), and let \(M = \binom{n}{2}!\). Let \(\mathbb{P}: \mathcal{P}(\Omega^{M}) \to [0,1]\) be defined for \(\omega \in \Omega^{M}\) so that
\[\mathbb{P}(\{\omega\}) = \begin{cases} \frac{1}{M!} & \text{ if } \omega \text{ consists of distinct edges} \\ 0 & \text{ otherwise}\end{cases}\]
This probability measure simply assigns uniform probability to all permutations of edges. Let \(\mathcal{F}_{0}= \{\emptyset, \Omega^{M}\}\) and \(\mathcal{F}_{i} = \{S \times \Omega^{M-i} : S \in \mathcal{P}(\Omega^{i})\}\) make up the filtration \(\mathcal{F}_{*} := (\mathcal{F}_{i})_{i=0}^{M}\).

\begin{definition}
    An \textbf{Erdős-Rényi random graph} on \(m \in \{0, \dots, M\}\) edges, \(G_{n, m}: \Omega \to \mathbb{G}_{n}\), is defined on \(\omega \in \Omega\) as
    \[G_{n, m}(\omega) = \{\omega_{1}, \dots, \omega_{m}\}.\]
    That is, it is a random graph sampled uniformly from all graphs on \(n\) vertices with \(m\) edges. 
\end{definition}

We can think of sampling a particular Erdős-Rényi graph as the result of picking an edge \(m\) times without replacement. This is captured by the process defined below:
\begin{definition}
    The \textbf{Erdős-Rényi graph process} is the stochastic process \((G_{n, i}: \Omega \to \mathbb{G}_{n})_{i=0}^{M}\) adapted to \(\mathcal{F}_{*}\).
\end{definition}

We would like to characterize properties about this process as \(n \to \infty\). To do so, we use the following concept:
\begin{definition}[\cite{diffeq_2020}]
     Let \((X_{n}: \Omega \to \mathbb{R})_{n \geq 1}\) be a stochastic process and let \((x_{n})_{n \geq 1}\subset \mathbb{R}\). Then \(X_{n} = x_{n}\) \textbf{asymptotically almost surely} if there exists \((\epsilon_{n})_{n \geq 1} \subset \mathbb{R}\) so that \(\epsilon_{n} = o(1)\) and
    \[\mathbb{P}((1 - \epsilon_{n})x_{n} \leq X_{n} \leq (1 + \epsilon_{n})x_{n}) \to 1\]
    as \(n \to \infty\). It may be abbreviated a.a.s..
\end{definition}

We would like to prove the following a.a.s. characterization of the number of connected components of a fixed size of an Erdős-Rényi random graph.

\begin{theorem}[\cite{diffeq_2020}]
    Let \(\kappa \geq 1\) and \(c \in \mathbb{R}^{\geq 0}\). Then a.a.s. the number of connected components of order \(k\), for \(1 \leq k \leq \kappa\), in \(G(n, \lfloor cn \rfloor)\) is
    \[\frac{k^{k-2}}{k!}(2c)^{k-1} e^{-2kc}n\]
\end{theorem}

To prove this theorem, we will apply the \textit{differential equation method}. This method works by approximating a random discrete process (in our case the number of connected components of size \(k\) in our random graph as we sample edges)with a continuous, deterministic, differential equation. We construct the differential equation using the expected change in our tracked random variable. We'll use some process-related heuristic to find a solution to the differential equation. We then show that our error incurred from our continuous approximation is small by bounding it from above and below by a supermartingale and submartingale respectively, then applying the following:

\begin{theorem}[Azuma-Hoeffding's Inequality \cite{diffeq_2020,azuma_1967}]
    \label{azuma}
    Let \((Y_{n})_{n \geq 0}\) be a supermartingale and suppose there exists real \(C \geq 0\) \(|Y_{n+1} - Y_{n}| \leq C\) a.s. for \(n \geq 0\). Then, for all \(\lambda \in \mathbb{R}^{\geq 0}\) and \(n \geq 1\),
    \[\mathbb{P}(Y_{n} - Y_{0} \geq  \lambda) \leq \exp\left(-\frac{\lambda^{2}}{2C^{2}n}\right)\]
\end{theorem}

% TODO: Proof?

The following will also be useful:

\begin{theorem}[Taylor's Theorem \cite{diffeq_2020}]
    \label{taylor}
    Let \(f \in C^{2}([a, b])\) for \(a < b \in \mathbb{R}\). Then there exists \(\tau \in (a, b)\) so that
    \[f(b) = f(a) + f'(a)(b-a) + \frac{f''(\tau)}{2}(b-a)^{2}.\]
\end{theorem}

Now we come back to our task of studying the number of fixed size connected components in an Erdős-Rényi graph. Recall we are given \(c \in
\mathbb{R}^{\geq 0}\) and suppose \(n \in \mathbb{N}\). Let \(i \leq \lfloor {cn} \rfloor\) and define \(Y_{k}(i)\) to be the number of components with exactly \(k\) vertices in \(G_{n, i}\).

We would like to show that \(Y_{k}(i) = n y_{k}(t_{i})\) a.a.s. for \(t_{i} := \frac{i}{n}\) and \(y_{k}(t) = \frac{k^{k-2}}{k!}(2t)^{k-1} e^{-2kt}\). In our Erdős-Rényi process, we'll let \(i\) progress to \(\lfloor {cn} \rfloor\) (i.e. let \(t \to c\)) to get the desired result. To prove \(Y_{k}(i) = n y_{k}(t_{i})\) a.a.s., we'll make a (good) guess that \(\epsilon_{n}(t) = n^{-1/3} e^{4 \kappa^{3} t}\) and show
\[n(y_{k}(t_{i}) - \epsilon_{n}(t_{i})) \leq Y_{k}(i) \leq n(y_{k}(t_{i}) + \epsilon_{n}(t_{i}))\]
tends to probability 1. This reasoning behind this choice of \(\epsilon_{n}\) will be clear as we proceed with the computations.

For now, we'll assume such a \(y_{k}\) exists and is in \(C^{2}([0, c])\), but take it to be unknown. Let \(\nu_{n}^{\pm} := \inf\limits \{i \geq 0 : \pm (Y_{k}(i) - n(y_{k}(t_{i})) > \epsilon_{n}(t_{i}))\}\). Define
\[Y_{k}^{\pm}(i) = Y_{k}(i \wedge \nu_{n}^{\pm}) - n(y_{k}(t_{i \wedge \nu_{n}^{\pm}}) \pm \epsilon_{n}(t_{i \wedge \nu_{n}^{\pm}})) \]
Effectively, we're freezing our error once \(Y_{k}(i)\) leaves our error bound. We will show that \(Y_{k}^{+}(i)\) is a supermartingale. Using that, we will use Azuma-Hoeffding (theorem \ref{azuma}) to show that \(\mathbb{P}(\nu_{n}^{+} = O(n) \text{ or } \nu_{n}^{-} = O(n)) = o(1)\). The corresponding computations for \(Y_{k}^{-}\) are symmetric.

Let's take a look at the one-step change of \(Y_{k}\) from \(i\) to \(i+1\), which we'll call \(\Delta Y_{k}(i)\). Given \(\mathcal{F}_{i}\) (that is, the first \(i\) choices of edges), we have the possible outcomes for new edge \(\omega_{i+1}\):
\begin{enumerate}
    \item \(\omega_{i + 1}\) connects a component of size \(k\) to one of size that's not \(k\). There are \(k Y_{k}(i) (n - k Y_{k}(i))\) such edges. In this case \(\Delta Y_{k}(i) = -1\).
    \item \(\omega_{i+1}\) connects a component of size \(k\) to another component of size \(k\). There are \(\frac{1}{2} k^{2} (Y_{k}(i) - 1) Y_{k}(i)\) such edges. In this case \(\Delta Y_{k}(i) = -2\).
    \item \(\omega_{i + 1}\) connects a component of size \(j\) and size \(k - j\) (for some \(j < k\)). There are \(j Y_{j}(i) (k - j) Y_{k-j}(i)\) such edges. In this case \(\Delta Y_{k}(i) = 1\).
    \item Otherwise,  \(\Delta Y_{k}(i) = 0\).
\end{enumerate}

There are \(\binom{n}{2} - i\) possible choices for edges. This means the probability of picking a particular edge that hasn't been chosen yet is
\[\frac{1}{\binom{n}{2} - i} = \frac{2}{n^{2} + n - 2i} \leq \frac{2}{n^{2}} \left(\frac{n^{2}}{n^{2} - n} \right) =  \frac{2}{n^{2}} \left(1 + O(\frac{1}{n})\right)\]

First looking at the contribution of cases 1 and 2 to \(\mathbb{E}(\Delta Y_{k}(i)|\mathcal{F}_{i})\)
\begin{align*}
    &-  \frac{2}{n^{2}} \left(1 + O(\frac{1}{n})\right) \Big[  n k Y_{k}(i) - k^{2} Y_{k}(i)^{2}   + k^{2} Y_{k}(i)^{2} - k^{2} Y_{k}(i)\Big] \\
    =&  -  \frac{2}{n^{2}} \left(1 + O(\frac{1}{n})\right) \Big[  n k Y_{k}(i) - k^{2} Y_{k}(i)\Big] \\
    =& \left(1 + O(\frac{1}{n})\right)  \Big[  -2 k \frac{Y_{k}(i)}{n} + k^{2} \frac{2Y_{k}(i)}{n^{2}}\Big] \\
    \leq& \left(1 + O(\frac{1}{n})\right)  \Big[  -2 k \frac{Y_{k}(i)}{n} + \kappa^{2} \frac{2}{n}\Big] \\
    \leq& -2 k \frac{Y_{k}(i)}{n} + O(\frac{1}{n}) + O(\frac{1}{n^{2}}) \\
    =& -2 k \frac{Y_{k}(i)}{n} + O(\frac{1}{n}) \\
\end{align*}
where we make use of the fact that \(Y_{k}(i) \leq n\) and \(k \leq \kappa\).

Next, we look at the contribution of case 3:
\begin{align*}
    & \frac{2}{n^{2}} \left(1 + O(\frac{1}{n})\right) \frac{1}{2}\sum\limits_{j=1}^{k-1} j Y_{j}(i) (k - j) Y_{k-j}(i) \\
    &=  \left(1 + O(\frac{1}{n})\right) \sum\limits_{j=1}^{k-1} j (k - j) \frac{Y_{j}(i)}{n} \frac{Y_{k-j}(i)}{n} \\
    &\leq \sum\limits_{j=1}^{k-1} \Big[  j (k - j) \frac{Y_{j}(i)}{n}  \frac{Y_{k-j}(i)}{n} \Big] + \kappa^{3} O(\frac{1}{n}) \\
    &=\sum\limits_{j=1}^{k-1} \Big[  j (k - j) \frac{Y_{j}(i)}{n}  \frac{Y_{k-j}(i)}{n} \Big]+ O(\frac{1}{n})
\end{align*}
where we again use the fact that \(Y_{j}(i), Y_{k-j}(i) \leq n\) and \(j, k-j \leq \kappa\). Thus, we have that
\[\mathbb{E}(\Delta Y_{k}(i)|\mathcal{F}_{i}) = -2 k \frac{Y_{k}(i)}{n} + \sum\limits_{j=1}^{k-1} \Big[ 2 j (k - j) \frac{Y_{j}(i)}{n}  \frac{Y_{k-j}(i)}{n} \Big] + O(\frac{1}{n})\]
Assuming \(i < \nu_{n}^{+}\), we have that
\begin{align*}
    \mathbb{E}(\Delta Y_{k}(i)|\mathcal{F}_{i}, [i < \nu_{n}^{+}]) &= -2 k (y_{k}(t_{i}) + \epsilon_{n}(t_{i})) + \sum\limits_{j=1}^{k-1} \Big[ j (k - j) (y_{j}(t_{i}) + \epsilon_{n}(t_{i})) (y_{k-j}(t_{i}) + \epsilon_{n}(t_{i})) \Big] + O(\frac{1}{n}) \\
    &\leq -2 k y_{k}(t_{i}) + \sum\limits_{j=1}^{k-1} \Big[ j (k - j) y_{j}(t_{i})y_{k-j}(t_{i})\Big] -2 k \epsilon_{n}(t_{i}) + \epsilon_{n}(t_{i}) 3 k^{3} + O(\frac{1}{n}) \\
    &\leq -2 k y_{k}(t_{i}) + \sum\limits_{j=1}^{k-1} \Big[ j (k - j) y_{j}(t_{i})y_{k-j}(t_{i})\Big] + \epsilon_{n}(t_{i}) 3 k^{3} + O(\frac{1}{n})
\end{align*}
using the fact that \(y_{l}(t) \leq 1\) for all \(1 \leq l \leq \kappa, 0 \leq t \leq c\) and \(0 \leq \epsilon_{n}(t) \leq 1\) for sufficiently large \(n\) for all \(0 \leq t \leq c\).

Now note that given \([i < \nu_{n}^{+}]\), \(\Delta Y_{k}^{+}(i) = \Delta Y_{k}(i) - (y_{k}(t_{i} + \frac{1}{n}) - y_{k}(t_{i}) + \epsilon_{n}(t_{i} + \frac{1}{n}) - \epsilon_{n}(t_{i}))\). We can simplify that last term using Taylor's Theorem (theorem \ref{taylor}) to get
\begin{align*}
    n(y_{k}(t_{i} + \frac{1}{n}) - y_{k}(t_{i})) &= n(\frac{y_{k}'(t_{i})}{n} + \frac{y_{k}''(\tau)}{n^{2}} \\
    &= y_{k}'(t_{i}) + O(\frac{1}{n})
\end{align*}
for some \(\tau \in (t_{i}, t_{i+1})\). Note that \(y_{k} \in C^{2}([0, c])\), so \(y_{k}''\) is continuous and it is bounded on \([0, c]\) (by the extreme value theorem). This justifies replacing \(n\frac{y_{k}''(\tau)}{n^{2}}\) with \(O(\frac{1}{n^{2}})\). Applying Taylor's Theorem to \(\epsilon_{n}\) as well:
\[n(\epsilon_{n}(t_{i} + \frac{1}{n}) - \epsilon_{n}(t_{i})) = \epsilon_{n}'(t_{i}) + O(\frac{1}{n})\]
Thus, we get
\begin{align*}
    \mathbb{E}(\Delta &Y^{+}_{k}(i)|\mathcal{F}_{i}, [i < \nu_{n}^{+}]) \\
    &= -2 k y_{k}(t_{i}) + \sum\limits_{j=1}^{k-1} \Big[ j (k - j) y_{j}(t_{i})y_{k-j}(t_{i})\Big]  + \epsilon_{n}(t_{i}) 3 k^{3} - y_{k}'(t_{i}) - \epsilon_{n}'(t_{i}) + O(\frac{1}{n})
\end{align*}
Note that if we choose our \((y_{k})_{k=1}^{\kappa}\) so that it solves the ODE system
\[y_{k}' = -2 k y_{k} + \sum\limits_{j=1}^{k-1} \Big[ j (k - j) y_{j}y_{k-j}\Big] \text{ } \forall 1 \leq k \leq \kappa \]

then we will get 
\begin{align*}
    \mathbb{E}(\Delta &Y^{+}_{k}(i)|\mathcal{F}_{i}, [i < \nu_{n}^{+}]) \\
    &= \epsilon_{n}(t_{i}) 3 k^{3} - \epsilon_{n}'(t_{i}) + O(\frac{1}{n})\\
    &=  3 k^{3} n^{-1/3} e^{4 \kappa^{3}t_{i}} - 4 \kappa^{3} n^{-1/3} e^{4 \kappa^{3}t_{i}} + O(\frac{1}{n}) \\
    &\leq  -\kappa^{3} n^{-1/3} e^{4 \kappa^{3}t_{i}} + O(\frac{1}{n}) \\
    &\leq 0
\end{align*}
for sufficiently large \(n\). Note that we applied \(k \leq \kappa\) above. From this computation the particular choice for the exponential part of our error term should now make sense. For the event \([i \geq \nu_{n}^{+}]\), \(Y^{+}_{k}\) is frozen and we have that \(\mathbb{E}(\Delta Y^{+}_{k}(i)|\mathcal{F}_{i}, [i \geq \nu_{n}^{+}]) = 0\). This means \(\mathbb{E}(\Delta Y^{+}_{k}(i)|\mathcal{F}_{i})\leq 0\) and is thus a supermartingale.

Now to solve our ODE system
\[y_{k}' = -2 k y_{k} + \sum\limits_{j=1}^{k-1} \Big[ 2 j (k - j) y_{j}y_{k-j}\Big] \text{ } \forall 1 \leq k \leq \kappa. \]
We claim (unsurprisingly) that \(y_{k}(t) = \frac{k^{k-2}}{k!}(2t)^{k-1} e^{-2kt}\) solves the ODE system.
\begin{claim} 
    \label{ode_claim}
    \(y_{k}(t) = \frac{k^{k-2}}{k!}(2t)^{k-1} e^{-2kt}\) is the unique solution to the ODE system
    \[y_{k}' = -2 k y_{k} + \sum\limits_{j=1}^{k-1} \Big[ j (k - j) y_{j}y_{k-j}\Big] \text{ } \forall 1 \leq k \leq \kappa. \]
\end{claim}
\begin{proof}[Proof of claim \ref{ode_claim}]
    For \(k = 1\), the relevant ODE has form
    \[y_{1}' = -2 y_{1}\]
    which is solved by \(y_{1} = e^{-2t}\). This agrees with our claimed solution after substituting \(k=1\). For \(k \geq 2\), we get
    \begin{align*}
        -2 k y_{k} + &\sum\limits_{j=1}^{k-1} \Big[ j (k - j) y_{j}y_{k-j}\Big] \\
        = &-2 k \frac{k^{k-2}}{k!}(2t)^{k-1} e^{-2kt} + \sum\limits_{j=1}^{k-1} \Big[ j (k - j) \frac{j^{j-2}}{j!}(2t)^{j-1} e^{-2jt} \frac{(k-j)^{(k-j)-2}}{(k-j)!}(2t)^{(k-j)-1} e^{-2(k-j)t}\Big] \\
        = &-2 k \frac{k^{k-2}}{k!}(2t)^{k-1} e^{-2kt} + \sum\limits_{j=1}^{k-1} \Big[  \frac{j^{j-1}(k-j)^{(k-j)-1} }{j!(k-j)!}(2t)^{k-2} e^{-2kt}\Big] \\
        = &-2 k \frac{k^{k-2}}{k!}(2t)^{k-1} e^{-2kt} + (2t)^{k-2} \frac{e^{-2kt}}{k!} \sum\limits_{j=1}^{k-1} \Big[ \binom{k}{j} j^{j-1}(k-j)^{(k-j)-1}\Big]
    \end{align*}

    We would like to show
    \[\sum\limits_{j=1}^{k-1} \Big[ \binom{k}{j} j^{j-1}(k-j)^{(k-j)-1}\Big] = 2(k-1)k^{k-2}.\]
    Then,
    \begin{align*}
        -2 k y_{k} + &\sum\limits_{j=1}^{k-1} \Big[ j (k - j) y_{j}y_{k-j}\Big] \\
        = &-2 k \frac{k^{k-2}}{k!}(2t)^{k-1} e^{-2kt} + (2t)^{k-2} \frac{e^{-2kt}}{k!} 2(k-1)k^{k-2} \\
        =& y_{k}'
    \end{align*}
    by product rule.

    We will prove the closed form of our sum by examining labelled trees on \([k]\). Observe that for \(1 \leq  j \leq k-1\)
    \begin{enumerate}
        \item we have \(\binom{k}{j}\) choices subsets \(J \subset [k]\) of size \(|J| = j\)
        \item we have \(j^{j-2}\) choices for labelled trees on \(J\) (Cayley's Formula)
        \item we have \((k-j)^{k-j-2}\) choices for labelled trees on \([k] \setminus J\)
        \item we have \(j(k-j)\) choices for edges between \(J\) and \([k] \setminus J\).
    \end{enumerate}
    Making a choice at each step will join two labelled trees to make a labelled tree on \([k]\). Each labelled tree on \([k]\) has \(k-1\) edges, so there are \(2(k-1)\) ways to create a given labelled tree on \([k]\) using the above procedure. The choices that produce a given labelled tree \(T\) on \([k]\) can be reverse engineered by
    \begin{enumerate}
        \item choosing one of the \(k-1\) edges of \(T\), call it \(e\)
        \item removing edge \(e\) from \(T\) and choosing one of the \(2\) remaining components as \(J\).
    \end{enumerate}
    Since there are \(k^{k-2}\) labelled trees on \([k]\), we have that
    \begin{align*}
        2(k-1)k^{k-2} &= \sum\limits_{j=1}^{k-1} \Big[ \binom{k}{j} j^{j-2}(k-j)^{(k-j)-2} j(k-j)\Big] \\
        &= \sum\limits_{j=1}^{k-1} \Big[ \binom{k}{j} j^{j-1}(k-j)^{(k-j)-1}\Big].
    \end{align*}

    Substituting back into our system and applying product rule gets us the desired result.
\end{proof}

So we have that for \(y_{k}(t) = \frac{k^{k-2}}{k!}(2t)^{k-1} e^{-2kt}\) and \(\epsilon_{n}(t) = n^{-1/3} e^{4 \kappa^{3}t}\), \((Y^{+}_{k}(i))_{i=0}^{\lfloor {cn} \rfloor}\) is a supermartingale.

We would now like to apply Azuma-Hoeffding (Theorem \ref{azuma}). First we need to show \(|\Delta Y_{k}^{+}(i)| = O(1)\) a.s. \(\forall 0 \leq i \leq \lfloor {cn} \rfloor\). This is straightforward since \(\Delta Y_{k}^{+}(i) = 0\) if \(i \geq \nu_{n}^{+}\), otherwise 
\begin{align*}
    |\Delta Y_{k}^{+}(i)| &= |\Delta Y_{k}(i) - (y_{k}(t_{i} + \frac{1}{n}) - y_{k}(t_{i}) + \epsilon_{n}(t_{i} + \frac{1}{n}) - \epsilon_{n}(t_{i}))| \\
    &= |\Delta Y_{k}(i) - y_{k}'(t_{i}) - \epsilon_{n}'(t_{i}) + O(\frac{1}{n})| \\
    &\leq |\Delta Y_{k}(i)| + |y_{k}'(t_{i})| + |\epsilon_{n}'(t_{i})| + O(\frac{1}{n}) \\
    &\leq 2 + O(1) + O(1) + O(\frac{1}{n}) \\
    &= O(1)
\end{align*}
because \(y_{k}\) and \(\epsilon_{n}\) are in \(C^{2}([0, c])\) and \(Y_{k}\) changes by at most \(-2\).

Now note that \(Y_{k}^{+}(i) > 0\) if and only if \(i \geq \nu_{n}^{+}\), so \(\mathbb{P}(i \geq \nu_{n}^{+}) = \mathbb{P}(Y_{k}^{+}(i) > 0)\) for any \(0 \leq i \leq \lfloor {cn} \rfloor\) Since,
\[
    Y_{k}(0) = \begin{cases} n & \text{ if } k =1 \\ 0 & \text{ otherwise} \end{cases} = ny_{k}(0)
\]
we have that
\[
    Y_{k}^{+}(0) = Y_{k}(0) - ny_{k}(0) - n\epsilon_{n}(0) = n\epsilon_{n}(0)
\]
for all \(1 \leq k \leq \kappa\). Applying Azuma's inequality (theorem \ref{azuma}) to the supermartingale \(Y_{k}^{+}\) at step \(m = \lfloor {cn} \rfloor\), we get
\begin{align*}
    \mathbb{P}(m \geq \nu_{n}^{+}) &= \mathbb{P}(Y_{k}^{+}(m) > 0) \\
    &= \mathbb{P}(Y_{k}^{+}(m) - Y_{k}^{+}(0) > -n\epsilon_{n}(0)) \\
    &= \mathbb{P}(Y_{k}^{+}(m) - Y_{k}^{+}(0) > -n^{2/3}) \\
    &\leq  \exp\left( - \frac{n^{4/3} }{2 C^{2}m} \right) \\
    &\leq \exp\left( - \frac{n^{1/3} }{2 C^{2}} \right) \\
    &= o(1)
\end{align*}
where we replaced \(m\) with \(\lfloor {cn} \rfloor\) and absorbed \(c\) into the \(C^{2}\).

Thus, \(\mathbb{P}(m < \nu_{n}^{+})\) a.a.s.. A symmetric calculation for \(Y_{k}^{-}\) gets us that \(\mathbb{P}(m < \nu_{n}^{-})\) a.a.s. as well. Finally, applying a union bound, we get that
\[\mathbb{P}(m \geq \nu_{n}^{+} \text{ or } m \geq \nu_{n}^{-}) \leq \mathbb{P}(m \geq \nu_{n}^{+}) + \mathbb{P}(m \geq \nu^{-}) = o(1) + o(1)\]

so
% \begin{align*}
%     ny_{k}(t_{m}) - n \epsilon_{n}(t_{m}) \\
%     ny_{k}(t_{m})( 1 - \frac{\epsilon_{n}(t_{m})}{y_{k}(t_{m})}) \\
%     ny_{k}(t_{m})( 1 - \frac{n^{-1/3} e^{4 \kappa^{3}t_{m}}}{\frac{k^{k-2}}{k!}(2t_{m})^{k-1} e^{-2kt_{m}}})
% \end{align*}
\begin{align*}
    \mathbb{P}(&ny_{k}(c)(1 - O(n^{-1/3})) \leq  Y_{k}(m) \leq ny_{k}(c)(1 + O(n^{-1/3}))) \\
    &= \mathbb{P}(n(y_{k}(c) - \epsilon_{n}(c)) \leq  Y_{k} \leq n(y_{k}(c) + \epsilon_{n}(c))) \\
    &= 1 - \mathbb{P}(m \geq \nu_{n}^{+} \text{ or } m \geq \nu_{n}^{-})) \\
    &\to  1\\
\end{align*}

Therefore the number of \(k\)-sized components of \(G_{n, \lfloor {cn} \rfloor}\) is a.a.s. \(ny_{k}(c)\)

% TODO: Complete application of Azuma to wrap it up.

% Since, we start out with a graph of no edges, 
% \[Y_{k}(0) = \begin{cases} n & \text{ if }k = 1 \\ 0 & \text{ otherwise} \end{cases}.\]
% Thus, \(y_{1}(0) = 1\) and \(y_{k}(0) = 0\) for \(k \geq 2\). 


% TODO:
% - Erdős-Rényi graph
% - Thm 1.2 (and defn of a.a.s.) about number of connected components
% - Erdős-Rényi process and equivalence at step to G(n, i)
% - What is differential equation Method and informally how we will use item
% - Useful theorems
%     - Taylor's Theorem (no proof)
%     - Azuma's inequality (with proof)
%         - Chernoff-Cramer bound
%         - Hoeffding's Lemma
%         - Azuma-Hoeffding inequality
%     - Cayley's formula (no proof)
% - stopping time for bad event
% - proof it is supermartingale
% - proof guessed solution solves differential equation
% - application of azuma-hoeffding
